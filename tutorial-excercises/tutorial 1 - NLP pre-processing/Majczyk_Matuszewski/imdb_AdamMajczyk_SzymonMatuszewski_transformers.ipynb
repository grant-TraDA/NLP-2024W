{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import pipeline\n",
    "import datetime\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "test_percentage = 1\n",
    "num_samples = int(len(dataset[\"test\"]) * test_percentage)\n",
    "\n",
    "\n",
    "# Define a custom dataset class if needed, or use dataset['test'] directly\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.texts = dataset[\"text\"]  # replace 'text' with the actual column name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "\n",
    "# Create a DataLoader\n",
    "test_sample = dataset[\"test\"].select(range(num_samples))\n",
    "test_dataset = CustomDataset(test_sample)  # use your dataset's test split\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"model\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted, true):\n",
    "    accuracy = accuracy_score(true, predicted)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true, predicted, average=\"weighted\"\n",
    "    )\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def add_result(df, model, predicted, true):\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(predicted, true)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"model\": [model],\n",
    "                    \"accuracy\": [accuracy],\n",
    "                    \"precision\": [precision],\n",
    "                    \"recall\": [recall],\n",
    "                    \"f1\": [f1],\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_llama = pipeline(\"text-classification\", model=\"yash3056/Llama-3.2-1B-imdb\", device=\"cuda\")\n",
    "\n",
    "# classify a few examples\n",
    "print(pipe_llama(\"The movie was great!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification in batches and show a progress bar\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Processing batches\"):\n",
    "    # Convert the batch to a list of texts\n",
    "    texts = batch  # Already in a format suitable for processing\n",
    "    predictions = pipe_llama(texts)  # Classify the batch\n",
    "    results.extend(predictions)  # Collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "true_labels = [example[\"label\"] for example in dataset[\"train\"]]\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = [example[\"label\"] for example in results]\n",
    "predicted_labels = [1 if label == \"LABEL_1\" else 0 for label in predicted_labels]\n",
    "\n",
    "predicted_labels[:5], true_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics\n",
    "result_df = add_result(result_df, \"Llama\", predicted_labels, true_labels)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\n",
    "    f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPNET_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mpnet_v2 = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"abhiramd22/finetuning-sentiment-model-mpnet-imdb\",\n",
    "    device=\"cuda\",\n",
    "    truncation=True,\n",
    ")\n",
    "# classify a few examples\n",
    "print(pipe_mpnet_v2(\"I love you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification in batches and show a progress bar\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Processing batches\"):\n",
    "    # Convert the batch to a list of texts\n",
    "    texts = batch  # Already in a format suitable for processing\n",
    "    predictions = pipe_mpnet_v2(texts)  # Classify the batch\n",
    "    results.extend(predictions)  # Collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mpnet_v2 = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"abhiramd22/finetuning-sentiment-model-mpnet-imdb\",\n",
    "    device=\"cuda\",\n",
    "    truncation=True,\n",
    ")\n",
    "# classify a few examples\n",
    "print(pipe_mpnet_v2(\"I love you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 1], [0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the true labels\n",
    "true_labels = [example[\"label\"] for example in dataset[\"train\"]]\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = [example[\"label\"] for example in results]\n",
    "predicted_labels = [0 if label == \"NEGATIVE\" else 1 for label in predicted_labels]\n",
    "\n",
    "predicted_labels[:5], true_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama</td>\n",
       "      <td>0.96356</td>\n",
       "      <td>0.963582</td>\n",
       "      <td>0.96356</td>\n",
       "      <td>0.96356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MPNet_v2</td>\n",
       "      <td>0.98140</td>\n",
       "      <td>0.981426</td>\n",
       "      <td>0.98140</td>\n",
       "      <td>0.98140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  accuracy  precision   recall       f1\n",
       "0     Llama   0.96356   0.963582  0.96356  0.96356\n",
       "0  MPNet_v2   0.98140   0.981426  0.98140  0.98140"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate metrics\n",
    "result_df = add_result(result_df, \"MPNet_v2\", predicted_labels, true_labels)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIDENOTE\n",
    "This result (98%) is too good to be true, I suspect some data leakage (the guy who fine-tuned the model may have used part of the test set for training). It should achieve ~96% accuracy on the test set, not 98%, according to the guy who fine-tuned the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe_gpt2 = pipeline(\"text-classification\", model=\"mnoukhov/gpt2-imdb-sentiment-classifier\", device=\"cuda\", truncation=True)\n",
    "\n",
    "# classify a few examples\n",
    "print(pipe_gpt2(\"The movie was great!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification in batches and show a progress bar\n",
    "# set TOKENIZERS_PARALLELISM=(true | false) to avoid warnings\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Processing batches\"):\n",
    "    # Convert the batch to a list of texts\n",
    "    texts = batch  # Already in a format suitable for processing\n",
    "    predictions = pipe_gpt2(texts)  # Classify the batch\n",
    "    results.extend(predictions)  # Collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "true_labels = [example[\"label\"] for example in dataset[\"train\"]]\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = [example[\"label\"] for example in results]\n",
    "predicted_labels = [0 if label == \"LABEL_0\" else 1 for label in predicted_labels]\n",
    "\n",
    "predicted_labels[:5], true_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = add_result(result_df, \"GPT2\", predicted_labels, true_labels)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\n",
    "    f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe_deberta = pipeline(\"text-classification\", model=\"dfurman/deberta-v3-base-imdb\", device=\"cuda\")\n",
    "\n",
    "# classify a few examples\n",
    "print(pipe_deberta(\"The movie was great!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classification in batches and show a progress bar\n",
    "results = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Processing batches\"):\n",
    "    # Convert the batch to a list of texts\n",
    "    texts = batch  # Already in a format suitable for processing\n",
    "    predictions = pipe_deberta(texts)  # Classify the batch\n",
    "    results.extend(predictions)  # Collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "true_labels = [example[\"label\"] for example in dataset[\"train\"]]\n",
    "# Get the predicted labels\n",
    "predicted_labels = [example[\"label\"] for example in results]\n",
    "predicted_labels = [0 if label == \"NEGATIVE\" else 1 for label in predicted_labels]\n",
    "\n",
    "predicted_labels[:5], true_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.93940</td>\n",
       "      <td>0.939530</td>\n",
       "      <td>0.93940</td>\n",
       "      <td>0.939396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeBERTa</td>\n",
       "      <td>0.95464</td>\n",
       "      <td>0.954916</td>\n",
       "      <td>0.95464</td>\n",
       "      <td>0.954633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model  accuracy  precision   recall        f1\n",
       "0     GPT2   0.93940   0.939530  0.93940  0.939396\n",
       "0  DeBERTa   0.95464   0.954916  0.95464  0.954633"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = add_result(result_df, \"DeBERTa\", predicted_labels, true_labels)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f\"results_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all files with results in name\n",
    "\n",
    "import os\n",
    "\n",
    "files = [f for f in os.listdir() if \"results\" in f]\n",
    "\n",
    "df = pd.concat([pd.read_csv(f) for f in files])\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df.to_csv(\"RESULTS_ALL_transformers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama</td>\n",
       "      <td>0.96356</td>\n",
       "      <td>0.963582</td>\n",
       "      <td>0.96356</td>\n",
       "      <td>0.963560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MPNet_v2</td>\n",
       "      <td>0.98140</td>\n",
       "      <td>0.981426</td>\n",
       "      <td>0.98140</td>\n",
       "      <td>0.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT2</td>\n",
       "      <td>0.93940</td>\n",
       "      <td>0.939530</td>\n",
       "      <td>0.93940</td>\n",
       "      <td>0.939396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeBERTa</td>\n",
       "      <td>0.95464</td>\n",
       "      <td>0.954916</td>\n",
       "      <td>0.95464</td>\n",
       "      <td>0.954633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  accuracy  precision   recall        f1\n",
       "0     Llama   0.96356   0.963582  0.96356  0.963560\n",
       "1  MPNet_v2   0.98140   0.981426  0.98140  0.981400\n",
       "0      GPT2   0.93940   0.939530  0.93940  0.939396\n",
       "1   DeBERTa   0.95464   0.954916  0.95464  0.954633"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCLAIMERS\n",
    "\n",
    "## Results\n",
    "This result (98%) is too good to be true, I suspect some data leakage (the guy who fine-tuned the model may have used part of the test set for training). It should achieve ~96% accuracy on the test set, not 98%, according to the guy who fine-tuned the model.\n",
    "\n",
    "## AI used for generating this notebook\n",
    " - ChatGPT\n",
    " - Github Copilot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
