{"cells":[{"cell_type":"markdown","id":"c9b09ee0","metadata":{},"source":["autorzy: Szymon Pawlonka, \n","Tomasz Krupiski"]},{"cell_type":"markdown","id":"af384d6e","metadata":{"id":"af384d6e"},"source":["## Introduction to NLP (Natural Language Processing)\n","<br>\n","漏 mgr in偶. Soveatin Kuntur / Winter School - Data Literacy\n","<br>\n","Warsaw, 06.02 - 10.02.2023\n","\n","漏 Anna Wr贸blewska, improvements and adding more metrics, and classifiers\n"]},{"cell_type":"markdown","id":"2d7ee401","metadata":{"id":"2d7ee401"},"source":["As we know from lecture, general pipeline of NLP is as follows:\n","<br>\n","1. Data Preprocessing\n","<br>\n","2. Data Processing\n","<br>\n","3. Output"]},{"cell_type":"markdown","id":"954d9269","metadata":{"id":"954d9269"},"source":["## Data Processing\n","\n","Now we come to second part of our experiment which is data processing. This task will be classification problem. Which means, we will ask our model to classify the sentiment. It is worth to mention that, you can use your clean dataset to do this part. <br>\n","\n","As for example reason, teacher will show how to use sklearn packages on this part."]},{"cell_type":"markdown","id":"ebfceaed","metadata":{"id":"ebfceaed"},"source":["## Dataset Description\n","The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.\n","\n","You can download the dataset in your workspace (or in your local computer) by doing\n","<br>\n","nltk.download('twitter_samples')\n"]},{"cell_type":"code","execution_count":10,"id":"238aa3f8","metadata":{"executionInfo":{"elapsed":16849,"status":"ok","timestamp":1728396364045,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"238aa3f8"},"outputs":[],"source":["import nltk                                # Python library for NLP\n","from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt            # library for visualization\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n","from nltk.classify import SklearnClassifier\n","# from wordcloud import WordCloud,STOPWORDS"]},{"cell_type":"markdown","id":"ce2a28e6","metadata":{"id":"ce2a28e6"},"source":["We can load the text fields of the positive and negative tweets by using the module's `strings()` method like this:"]},{"cell_type":"code","execution_count":11,"id":"oJ0QIR-GpwUO","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4492,"status":"ok","timestamp":1728396368527,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"oJ0QIR-GpwUO","outputId":"9bbd6e15-ead4-45ab-9b42-e0fc28f022ea"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to\n","[nltk_data]     C:\\Users\\spawl\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# downloads sample twitter dataset.\n","nltk.download('twitter_samples')"]},{"cell_type":"code","execution_count":12,"id":"2861bdc0","metadata":{"executionInfo":{"elapsed":8430,"status":"ok","timestamp":1728396381711,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"2861bdc0"},"outputs":[],"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"]},{"cell_type":"markdown","id":"5593a249","metadata":{"id":"5593a249"},"source":["To make it easier, let us used pandas dataframe"]},{"cell_type":"code","execution_count":13,"id":"e0cdbbea","metadata":{"executionInfo":{"elapsed":384,"status":"ok","timestamp":1728396384769,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"e0cdbbea"},"outputs":[],"source":["# Create a dataframe from positive tweets\n","pos_tweet = pd.DataFrame(all_positive_tweets, columns=['Tweet'])\n","# Add a column to dataframe for positive sentiment value 1\n","pos_tweet['Sentiment'] = 1\n","# Create a temporary dataframe for negative tweets\n","neg_tweet = pd.DataFrame(all_negative_tweets, columns=['Tweet'])\n","# Add a column to temporary dataframe for negative sentiment value 0\n","neg_tweet['Sentiment'] = 0\n","# Combe positive and negative tweets in one single dataframe\n","#df = pos_tweet.append(neg_tweet, ignore_index=True)\n","df = pd.concat([neg_tweet, pos_tweet], ignore_index=True)\n","\n","df = df.sample(frac = 1)\n","df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":14,"id":"1cd76d89","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":794,"status":"ok","timestamp":1728396389067,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"1cd76d89","outputId":"003fe141-3092-41b2-8811-aad8f6a8b18c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@walls @UberUK @cornettouk I can't see where i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@CranksFood I love Sour Dough for a treat :)</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I want to sleep why am I not tired :((((</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@CrazyGreen16 HAHAHA apink this song wont win ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@no1_razorstan then talk to me. I can help you :)</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>That guy smells very uuughhhh. Grabe ka power :(</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>A chover :(</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>@leebsfc11 @CoachLockey real trick shots. :)</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>@MissHayley1988 @djdarrenjones :( *hugs*</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>I'm playing Brain Dots : ) #BrainDots\\nhttp://...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows  2 columns</p>\n","</div>"],"text/plain":["                                                  Tweet  Sentiment\n","0     @walls @UberUK @cornettouk I can't see where i...          0\n","1          @CranksFood I love Sour Dough for a treat :)          1\n","2              I want to sleep why am I not tired :((((          0\n","3     @CrazyGreen16 HAHAHA apink this song wont win ...          0\n","4     @no1_razorstan then talk to me. I can help you :)          1\n","...                                                 ...        ...\n","9995   That guy smells very uuughhhh. Grabe ka power :(          0\n","9996                                        A chover :(          0\n","9997       @leebsfc11 @CoachLockey real trick shots. :)          1\n","9998           @MissHayley1988 @djdarrenjones :( *hugs*          0\n","9999  I'm playing Brain Dots : ) #BrainDots\\nhttp://...          1\n","\n","[10000 rows x 2 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","id":"3c9dbb5e","metadata":{"id":"3c9dbb5e"},"source":["Next, we'll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets"]},{"cell_type":"code","execution_count":15,"id":"1c42aaf1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1728396395228,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"1c42aaf1","outputId":"465ebb5f-0e95-4d2c-a931-b983dbd3feb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset size: (10000, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet</th>\n","    </tr>\n","    <tr>\n","      <th>Sentiment</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Tweet\n","Sentiment       \n","0           5000\n","1           5000"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Displaying shape of dataset\n","print('Dataset size:',df.shape)\n","df.groupby('Sentiment').count()"]},{"cell_type":"code","execution_count":16,"id":"0b2d0acb","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":428,"status":"ok","timestamp":1728396398642,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"0b2d0acb","outputId":"b1bd8b1a-1e4b-411a-bfd8-a9c911789ca3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4296</th>\n","      <td>@OscarTrue89 no sorry I'm fully booked :-( xx</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3287</th>\n","      <td>And that's how my 360hrs of \"summer\" ends........</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>474</th>\n","      <td>I was jealous because you're my bestfriend and...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>614</th>\n","      <td>@JadenChaos I'm about to start grinding YT :)</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1220</th>\n","      <td>Hammering down here right now......looks set f...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3032</th>\n","      <td>This Quite Phenomenal Offer On Bosch Washing M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3369</th>\n","      <td>There's a huge bag of presents from Luke and I...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6026</th>\n","      <td>Hey @CBuchanan68, can I maybe get a picture wi...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5457</th>\n","      <td>@CruiseLineUK @AzamaraVoyages can't tell you h...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6663</th>\n","      <td>@SarahThomas49 Hi Sarah, all our delish ranges...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9000 rows  2 columns</p>\n","</div>"],"text/plain":["                                                  Tweet  Sentiment\n","4296      @OscarTrue89 no sorry I'm fully booked :-( xx          0\n","3287  And that's how my 360hrs of \"summer\" ends........          0\n","474   I was jealous because you're my bestfriend and...          1\n","614       @JadenChaos I'm about to start grinding YT :)          1\n","1220  Hammering down here right now......looks set f...          0\n","...                                                 ...        ...\n","3032  This Quite Phenomenal Offer On Bosch Washing M...          1\n","3369  There's a huge bag of presents from Luke and I...          0\n","6026  Hey @CBuchanan68, can I maybe get a picture wi...          0\n","5457  @CruiseLineUK @AzamaraVoyages can't tell you h...          0\n","6663  @SarahThomas49 Hi Sarah, all our delish ranges...          1\n","\n","[9000 rows x 2 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Splitting the dataset into train and test set\n","train, test = train_test_split(df,test_size = 0.1)\n","train"]},{"cell_type":"code","execution_count":17,"id":"6fc362de","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":473,"status":"ok","timestamp":1728396401516,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"6fc362de","outputId":"dcecc692-b642-486a-eee2-2d2ca789b9bd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2640</th>\n","      <td>Happy Birthday @TobiWanDOTA ! Never forget. :)...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>700</th>\n","      <td>@jess_lakeland Sorry to hear this Jess :( The ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3350</th>\n","      <td>@fromislet @_tmazur YEP - I would vote for her...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8819</th>\n","      <td>:(: (Vine by luke clips) https://t.co/UB4OLW7ACW</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9810</th>\n","      <td>@GabyWhittlenew no way!!!! Aww :( such a funny...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9416</th>\n","      <td>Yet another #RandomRestart #RandomReboot of #L...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2279</th>\n","      <td>@thespringsteens looks like great fun! :)</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9489</th>\n","      <td>@PinkPeoniesBlog @helloprettysa Market on Main...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9747</th>\n","      <td>Short weekend ahead :(</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8513</th>\n","      <td>Srsly today was like the best day ever :))</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows  2 columns</p>\n","</div>"],"text/plain":["                                                  Tweet  Sentiment\n","2640  Happy Birthday @TobiWanDOTA ! Never forget. :)...          1\n","700   @jess_lakeland Sorry to hear this Jess :( The ...          0\n","3350  @fromislet @_tmazur YEP - I would vote for her...          1\n","8819   :(: (Vine by luke clips) https://t.co/UB4OLW7ACW          0\n","9810  @GabyWhittlenew no way!!!! Aww :( such a funny...          0\n","...                                                 ...        ...\n","9416  Yet another #RandomRestart #RandomReboot of #L...          0\n","2279          @thespringsteens looks like great fun! :)          1\n","9489  @PinkPeoniesBlog @helloprettysa Market on Main...          1\n","9747                             Short weekend ahead :(          0\n","8513         Srsly today was like the best day ever :))          1\n","\n","[1000 rows x 2 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["test"]},{"cell_type":"code","execution_count":18,"id":"01130dcf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":38101,"status":"ok","timestamp":1728396442020,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"01130dcf","outputId":"73971b7c-6717-416c-c411-05cdef7dd6fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive words\n"]},{"ename":"NameError","evalue":"name 'WordCloud' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[18], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mwordcloud_draw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m wordcloud_draw(train_neg)\n","Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mwordcloud_draw\u001b[1;34m(data, color)\u001b[0m\n\u001b[0;32m      7\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data)\n\u001b[0;32m      8\u001b[0m cleaned_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      9\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word\n\u001b[0;32m     10\u001b[0m                             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m                             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m                             \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRT\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m                         ])\n\u001b[1;32m---> 14\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(stopwords\u001b[38;5;241m=\u001b[39mSTOPWORDS,\n\u001b[0;32m     15\u001b[0m                   background_color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m     16\u001b[0m                   width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m,\n\u001b[0;32m     17\u001b[0m                   height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m     18\u001b[0m                  )\u001b[38;5;241m.\u001b[39mgenerate(cleaned_word)\n\u001b[0;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m13\u001b[39m))\n\u001b[0;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud)\n","\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"]}],"source":["train_pos = train[ train['Sentiment'] == 1]\n","train_pos = train_pos['Tweet']\n","train_neg = train[ train['Sentiment'] == 0]\n","train_neg = train_neg['Tweet']\n","\n","def wordcloud_draw(data, color = 'black'):\n","    words = ' '.join(data)\n","    cleaned_word = \" \".join([word for word in words.split()\n","                            if 'http' not in word\n","                                and not word.startswith('@')\n","                                and not word.startswith('#')\n","                                and word != 'RT'\n","                            ])\n","    wordcloud = WordCloud(stopwords=STOPWORDS,\n","                      background_color=color,\n","                      width=2500,\n","                      height=2000\n","                     ).generate(cleaned_word)\n","    plt.figure(1,figsize=(13, 13))\n","    plt.imshow(wordcloud)\n","    plt.axis('off')\n","    plt.show()\n","\n","print(\"Positive words\")\n","wordcloud_draw(train_pos,'white')\n","print(\"Negative words\")\n","wordcloud_draw(train_neg)"]},{"cell_type":"code","execution_count":12,"id":"E7JVijrhqYOG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1728396442022,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"E7JVijrhqYOG","outputId":"75e13b3f-2d68-48f8-de67-31929ee6cfcc"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\spawl\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping corpora\\stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('stopwords')"]},{"cell_type":"code","execution_count":13,"id":"bd5c1742","metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1728396442398,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"bd5c1742"},"outputs":[],"source":["\n","tweets = []\n","stopwords_set = set(stopwords.words(\"english\"))\n","\n","for index, row in train.iterrows():\n","    words_filtered = [e.lower() for e in row.Tweet.split() if len(e) >= 3]\n","    words_cleaned = [word for word in words_filtered\n","        if 'http' not in word\n","        and not word.startswith('@')\n","        and not word.startswith('#')\n","        and word != 'RT']\n","    words_without_stopwords = [word for word in words_cleaned if not word in stopwords_set]\n","    tweets.append((words_without_stopwords, row.Sentiment))\n","\n","test_pos = test[ test['Sentiment'] == 1]\n","test_pos = test_pos['Tweet']\n","test_neg = test[ test['Sentiment'] == 0]\n","test_neg = test_neg['Tweet']"]},{"cell_type":"code","execution_count":14,"id":"794f4da6","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728396442398,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"794f4da6"},"outputs":[],"source":["# Extracting word features\n","def get_words_in_tweets(tweets):\n","    all = []\n","    for (words, sentiment) in tweets:\n","        all.extend(words)\n","    return all\n","\n","def get_word_features(wordlist):\n","    wordlist = nltk.FreqDist(wordlist)\n","    features = wordlist.keys()\n","    return features\n","w_features = get_word_features(get_words_in_tweets(tweets))\n","\n","def extract_features(document):\n","    document_words = set(document)\n","    features = {}\n","    for word in w_features:\n","        features['contains(%s)' % word] = (word in document_words)\n","    return features"]},{"cell_type":"code","execution_count":15,"id":"ccf53207","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":811},"executionInfo":{"elapsed":17972,"status":"ok","timestamp":1728396461311,"user":{"displayName":"Anna Wr贸blewska","userId":"05889376538112120938"},"user_tz":-120},"id":"ccf53207","outputId":"acc770ee-1fc3-4e9c-be28-715c26eb6852"},"outputs":[{"ename":"NameError","evalue":"name 'WordCloud' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwordcloud_draw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_features\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mwordcloud_draw\u001b[1;34m(data, color)\u001b[0m\n\u001b[0;32m      7\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data)\n\u001b[0;32m      8\u001b[0m cleaned_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      9\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word\n\u001b[0;32m     10\u001b[0m                             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m                             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m                             \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRT\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m                         ])\n\u001b[1;32m---> 14\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m(stopwords\u001b[38;5;241m=\u001b[39mSTOPWORDS,\n\u001b[0;32m     15\u001b[0m                   background_color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m     16\u001b[0m                   width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m,\n\u001b[0;32m     17\u001b[0m                   height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m     18\u001b[0m                  )\u001b[38;5;241m.\u001b[39mgenerate(cleaned_word)\n\u001b[0;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m13\u001b[39m))\n\u001b[0;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud)\n","\u001b[1;31mNameError\u001b[0m: name 'WordCloud' is not defined"]}],"source":["wordcloud_draw(w_features)"]},{"cell_type":"code","execution_count":16,"id":"61818413","metadata":{"id":"61818413"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training the Naive Bayes classifier\u001b[39;00m\n\u001b[0;32m      2\u001b[0m training_set \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mclassify\u001b[38;5;241m.\u001b[39mapply_features(extract_features,tweets)\n\u001b[1;32m----> 3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNaiveBayesClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\spawl\\Anaconda3\\envs\\NLP\\Lib\\site-packages\\nltk\\classify\\naivebayes.py:214\u001b[0m, in \u001b[0;36mNaiveBayesClassifier.train\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    211\u001b[0m label_freqdist[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname, fval \u001b[38;5;129;01min\u001b[39;00m featureset\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# Increment freq(fval|label, fname)\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     \u001b[43mfeature_freqdist\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfval\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# Record that fname can take the value fval.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     feature_values[fname]\u001b[38;5;241m.\u001b[39madd(fval)\n","File \u001b[1;32mc:\\Users\\spawl\\Anaconda3\\envs\\NLP\\Lib\\site-packages\\nltk\\probability.py:126\u001b[0m, in \u001b[0;36mFreqDist.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03mOverride ``Counter.__setitem__()`` to invalidate the cached N\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Training the Naive Bayes classifier\n","training_set = nltk.classify.apply_features(extract_features,tweets)\n","classifier = nltk.NaiveBayesClassifier.train(training_set)"]},{"cell_type":"code","execution_count":null,"id":"b6911d00","metadata":{"colab":{"background_save":true},"id":"b6911d00"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Negative]: 500/415 \n","[Positive]: 500/327 \n"]}],"source":["neg_cnt = 0\n","pos_cnt = 0\n","for obj in test_neg:\n","    res =  classifier.classify(extract_features(obj.split()))\n","    if(res == 0):\n","        neg_cnt = neg_cnt + 1\n","for obj in test_pos:\n","    res =  classifier.classify(extract_features(obj.split()))\n","    if(res == 1):\n","        pos_cnt = pos_cnt + 1\n","\n","print('[Negative]: %s/%s '  % (len(test_neg),neg_cnt))\n","print('[Positive]: %s/%s '  % (len(test_pos),pos_cnt))"]},{"cell_type":"markdown","id":"9531b265","metadata":{"id":"9531b265"},"source":["## Useful links to start NLP\n","<br>\n","https://realpython.com/python-nltk-sentiment-analysis/"]},{"cell_type":"code","execution_count":18,"id":"a9fc4c8f","metadata":{"colab":{"background_save":true},"id":"a9fc4c8f"},"outputs":[{"ename":"NameError","evalue":"name 'classifier' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     10\u001b[0m     true_labels\u001b[38;5;241m.\u001b[39mappend(test\u001b[38;5;241m.\u001b[39mloc[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m obj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m     predicted_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43mclassifier\u001b[49m\u001b[38;5;241m.\u001b[39mclassify(extract_features(obj\u001b[38;5;241m.\u001b[39msplit())))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate and print the confusion matrix\u001b[39;00m\n\u001b[0;32m     14\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(true_labels, predicted_labels)\n","\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"]}],"source":["# prompt: print confusion matrix\n","\n","from sklearn.metrics import confusion_matrix\n","\n","# Create lists to store true labels and predicted labels\n","true_labels = []\n","predicted_labels = []\n","\n","for obj in test['Tweet']:\n","    true_labels.append(test.loc[test['Tweet'] == obj, 'Sentiment'].iloc[0])\n","    predicted_labels.append(classifier.classify(extract_features(obj.split())))\n","\n","# Calculate and print the confusion matrix\n","cm = confusion_matrix(true_labels, predicted_labels)\n","print(\"Confusion Matrix:\")\n","print(cm)\n"]},{"cell_type":"code","execution_count":null,"id":"MKBSyEE_sbdm","metadata":{"colab":{"background_save":true},"id":"MKBSyEE_sbdm"},"outputs":[],"source":["#measure F1 score, and recall, and precision\n","#build other model, e.g. random forest for this task"]},{"cell_type":"code","execution_count":17,"id":"4uiIyUpgsl-b","metadata":{"colab":{"background_save":true},"id":"4uiIyUpgsl-b"},"outputs":[{"ename":"NameError","evalue":"name 'true_labels' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, recall_score, precision_score\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate F1 score\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(\u001b[43mtrue_labels\u001b[49m, predicted_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Use 'weighted' for multi-class problems\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1 Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, f1)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate recall\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'true_labels' is not defined"]}],"source":["# prompt: measure F1 score, and recall, and precision\n","\n","from sklearn.metrics import f1_score, recall_score, precision_score\n","\n","# Calculate F1 score\n","f1 = f1_score(true_labels, predicted_labels, average='weighted')  # Use 'weighted' for multi-class problems\n","print(\"F1 Score:\", f1)\n","\n","# Calculate recall\n","recall = recall_score(true_labels, predicted_labels, average='weighted')\n","print(\"Recall:\", recall)\n","\n","# Calculate precision\n","precision = precision_score(true_labels, predicted_labels, average='weighted')\n","print(\"Precision:\", precision)\n"]},{"cell_type":"code","execution_count":19,"id":"Dg5oNZTlsuwS","metadata":{"colab":{"background_save":true},"id":"Dg5oNZTlsuwS"},"outputs":[{"ename":"NameError","evalue":"name 'true_labels' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43mtrue_labels\u001b[49m, predicted_labels)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n","\u001b[1;31mNameError\u001b[0m: name 'true_labels' is not defined"]}],"source":["# prompt: add another metric for assesing the model\n","\n","from sklearn.metrics import accuracy_score\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","print(\"Accuracy:\", accuracy)\n"]},{"cell_type":"code","execution_count":20,"id":"ib-mXzBm3vYP","metadata":{"colab":{"background_save":true},"id":"ib-mXzBm3vYP"},"outputs":[],"source":["# prompt: based on tweets prepare training set, without nltk library\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.naive_bayes import GaussianNB\n","\n","from sklearn import svm\n","from sklearn.linear_model import BayesianRidge, LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n","\n","# Assuming 'train' and 'test' DataFrames are already defined with 'Tweet' and 'Sentiment' columns\n","\n","# Prepare the training data\n","X_train = train['Tweet'].tolist()\n","y_train = train['Sentiment'].tolist()\n","\n","# Prepare the test data\n","X_test = test['Tweet'].tolist()\n","y_test = test['Sentiment'].tolist()\n","\n","\n","# Create a CountVectorizer to convert text into numerical features\n","vectorizer = CountVectorizer(stop_words='english')  # Use stop words for better performance\n","\n","# Fit and transform the training data\n","X_train_vectorized = vectorizer.fit_transform(X_train)\n","\n","# Transform the test data (using the same vocabulary as the training data)\n","X_test_vectorized = vectorizer.transform(X_test)\n","\n","models = [\n","    LogisticRegression(max_iter=1000),\n","    RandomForestRegressor(max_depth=2, random_state=0),\n","    svm.SVR(kernel='linear'),\n","    # BayesianRidge()\n","]\n","\n","scores = []\n","\n","for model in models:\n","    if isinstance(model, BayesianRidge):\n","        model.fit(X_train_vectorized.toarray(), y_train)\n","    else:\n","        model.fit(X_train_vectorized, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = model.predict(X_test_vectorized)\n","    y_pred = [1 if x >= 0.5 else 0 for x in y_pred]  # Convert probabilities to binary predictions\n","\n","\n","    # Evaluate the model\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred, average='weighted')\n","    recall = recall_score(y_test, y_pred, average='weighted')\n","    precision = precision_score(y_test, y_pred, average='weighted')\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    scores.append({\n","        'Model': model.__class__.__name__,\n","        'Accuracy': accuracy,\n","        'F1 Score': f1,\n","        'Recall': recall,\n","        'Precision': precision,\n","        'Confusion Matrix': cm\n","    })"]},{"cell_type":"code","execution_count":21,"id":"d5f095ce","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                   Model  Accuracy  F1 Score  Recall  Precision  \\\n","0     LogisticRegression     0.749  0.748138   0.749   0.748924   \n","1  RandomForestRegressor     0.636  0.585205   0.636   0.707486   \n","2                    SVR     0.712  0.710717   0.712   0.711738   \n","\n","           Confusion Matrix  \n","0  [[426, 109], [142, 323]]  \n","1   [[509, 26], [338, 127]]  \n","2  [[411, 124], [164, 301]]  \n"]}],"source":["scores_df = pd.DataFrame(scores)\n","print(scores_df)"]},{"cell_type":"code","execution_count":null,"id":"TXpivWty5OF7","metadata":{"colab":{"background_save":true},"id":"TXpivWty5OF7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.665\n","F1 Score: 0.6506016431109595\n","Recall: 0.665\n","Precision: 0.6975659870396712\n","Confusion Matrix:\n"," [[434  66]\n"," [269 231]]\n"]}],"source":["# prompt: the same as above but with random forest\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Assuming 'train' and 'test' DataFrames are already defined with 'Tweet' and 'Sentiment' columns\n","\n","# Prepare the training data\n","X_train = train['Tweet'].tolist()\n","y_train = train['Sentiment'].tolist()\n","\n","# Prepare the test data\n","X_test = test['Tweet'].tolist()\n","y_test = test['Sentiment'].tolist()\n","\n","\n","# Create a CountVectorizer to convert text into numerical features\n","vectorizer = CountVectorizer(stop_words='english')  # Use stop words for better performance\n","\n","# Fit and transform the training data\n","X_train_vectorized = vectorizer.fit_transform(X_train)\n","\n","# Transform the test data (using the same vocabulary as the training data)\n","X_test_vectorized = vectorizer.transform(X_test)\n","\n","# Train a Random Forest model\n","model = RandomForestClassifier(max_depth=2, random_state=0)\n","model.fit(X_train_vectorized, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test_vectorized)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred, average='weighted')\n","recall = recall_score(y_test, y_pred, average='weighted')\n","precision = precision_score(y_test, y_pred, average='weighted')\n","cm = confusion_matrix(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"F1 Score:\", f1)\n","print(\"Recall:\", recall)\n","print(\"Precision:\", precision)\n","print(\"Confusion Matrix:\\n\", cm)\n"]},{"cell_type":"code","execution_count":null,"id":"bEvvlkHBtVnY","metadata":{"colab":{"background_save":true},"id":"bEvvlkHBtVnY"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n","                                                  Tweet  \\\n","7049  @twentyonepilots @fujirock_jp tylers hipster g...   \n","7473                    @mussshiii @Takafofo so wat??:)   \n","444   I remember when Fab Four had a 24 hour call, D...   \n","1436             @Wilma2207fWilma haha! Thank you! :)    \n","7917  A perfect triangle will win the ball game. :) ...   \n","...                                                 ...   \n","9827  Stats for the week have arrived. 1 new followe...   \n","7311  @WizKhallista it's about the egg, water and bl...   \n","9227            is allergic to cats... itchy throat. :(   \n","9450             Now I'm sad :( https://t.co/Ribf3SkrDI   \n","8441           My body still hurts af. Just got home :)   \n","\n","                                       Sentiment Scores  Compound Score  \\\n","7049  {'neg': 0.0, 'neu': 0.554, 'pos': 0.446, 'comp...          0.6166   \n","7473  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n","444   {'neg': 0.319, 'neu': 0.554, 'pos': 0.128, 'co...         -0.5390   \n","1436  {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'comp...          0.8436   \n","7917  {'neg': 0.0, 'neu': 0.432, 'pos': 0.568, 'comp...          0.8885   \n","...                                                 ...             ...   \n","9827  {'neg': 0.294, 'neu': 0.706, 'pos': 0.0, 'comp...         -0.6611   \n","7311  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...          0.0000   \n","9227  {'neg': 0.643, 'neu': 0.357, 'pos': 0.0, 'comp...         -0.7351   \n","9450  {'neg': 0.667, 'neu': 0.333, 'pos': 0.0, 'comp...         -0.7184   \n","8441  {'neg': 0.237, 'neu': 0.534, 'pos': 0.229, 'co...         -0.0258   \n","\n","     Vader Sentiment  \n","7049        Positive  \n","7473         Neutral  \n","444         Negative  \n","1436        Positive  \n","7917        Positive  \n","...              ...  \n","9827        Negative  \n","7311         Neutral  \n","9227        Negative  \n","9450        Negative  \n","8441         Neutral  \n","\n","[9000 rows x 4 columns]\n","Vader Sentiment\n","Positive    5106\n","Negative    3278\n","Neutral      616\n","Name: count, dtype: int64\n"]}],"source":["# prompt: add sentiment analysis with Vader library\n","\n","!pip install vaderSentiment\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","analyzer = SentimentIntensityAnalyzer()\n","\n","# Function to get sentiment scores\n","def get_sentiment_scores(text):\n","    return analyzer.polarity_scores(text)\n","\n","# Apply sentiment analysis to the 'Tweet' column\n","train['Sentiment Scores'] = train['Tweet'].apply(get_sentiment_scores)\n","\n","# Extract compound score (overall sentiment)\n","train['Compound Score'] = train['Sentiment Scores'].apply(lambda x: x['compound'])\n","\n","# Assign sentiment labels based on compound score\n","train['Vader Sentiment'] = train['Compound Score'].apply(lambda x: 'Positive' if x >= 0.05 else ('Negative' if x <= -0.05 else 'Neutral'))\n","\n","\n","# Print the DataFrame with sentiment scores and labels\n","print(train[['Tweet', 'Sentiment Scores', 'Compound Score', 'Vader Sentiment']])\n","\n","# You can now analyze the 'Vader Sentiment' column to see how the Vader library categorized the tweets.\n","\n","# Example: Analyze the distribution of sentiment labels\n","print(train['Vader Sentiment'].value_counts())\n"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"NLP","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
