{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QygLD_EHhatn"
      },
      "outputs": [],
      "source": [
        "# Wasielewska Maja\n",
        "# Jakub Kubacki\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from gensim.models import KeyedVectors\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LtHwICiMvyaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    tokens = text.lower().split()\n",
        "\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "X_train_cleaned = [preprocess_text(text) for text in X_train]\n",
        "X_test_cleaned = [preprocess_text(text) for text in X_test]\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.round(y_pred).astype(int)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    return precision, recall, f1\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "split_dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "train_data = split_dataset['train']\n",
        "test_data = split_dataset['test']\n",
        "\n",
        "X_train = [item['text'] for item in train_data]\n",
        "y_train = [item['label'] for item in train_data]\n",
        "X_test = [item['text'] for item in test_data]\n",
        "y_test = [item['label'] for item in test_data]\n",
        "\n",
        "X_train = [preprocess_text(text) for text in X_train]\n",
        "X_test = [preprocess_text(text) for text in X_test]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "results_table = pd.DataFrame(columns=[\"Model\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
        "\n",
        "print(X_train_cleaned[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CvGjuMgAdq_",
        "outputId": "2d6f2eff-855e-47f2-e6f0-31d22103817c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['take say lightly ive seen many many film reviewed great deal print tell film single funniest scene ever seen movie might want listen lot diversity opinion make incredibly stupid movie funny didnt get well cant blame much scene speak come minute mark involves dead convict shackled john candy point found film dumb confusing beginning lose scene came laughed hard peed pant movie ever done project began going berserk supposed sctv movie remember announced time went cast whittled john candy joe flaherty eugene levy also must regime change universal shot upon released shown nearly zero theater watching second time listened theme song actually flaunt incomprehensible plot lyric relaxed logic nerve figured going aside aforementioned routine going berserk many hilarious scene recommend almost stooge flick except much funnier director david steinberg razor sharp timing must laughing candy basically charge never funnier plot device explanatory scene thrown window absolutely run wild flaherty levy follow effortlessly plot plot like animal house plot yeah script uneven little slow start know however well appreciate full sctv style craziness transpires stupid stupid purpose need remember see see discover funniest scene time', 'saw movie first second voice tr took journey well disliked big glued thumb beginning absurd humor gordious look sissy actor know played young great uma br br two people cinema went half hour friend always test watch movie like good one friend enjoyed maximum hilarious laugh sadness realistic police normalos u fan trobbins booksi found well done thought robbins would also approve though idea like film notbr br would love see cut stuff heard gu v sand take lot scene firsttime viewer producer well still artistic movie much short though one time favorite aware majority people cant stand kind movie assume people enjoy film whatever think pity hopefully come day dvd full material hoping see crispian keanu expecting see baby allbr br chance see think twice enjoy made choice watch', 'watched movie seeing broadway love broadway musical love movie watched movie like related broadway show avid reader seen happens book turned movie developed philosophy really early assume movie going based book musical case story line may similar different watch isbr br danced year make choice good dancer picking chorus work local production child etc wasnt super talentedi however super talented show rider told dance instructor trainer spent several month year farm state make choice turned needed move dancing two hour fourfive day week riding hour day day week dedicate one dearly love dancing love movie lot ballet dance movie chose watch movie great movie raw emotion human interaction power anticipation heartbreak work really hard get something want get love movie love broadway musical', 'really wish making comedy people actually tried make funny film sit nothing special watching say alright boring watch gave audience joke laugh entertainment mean actually get something ok story movie wanted tootsie instead failedbr br gave', 'well gave movie better thirdspace good beginning far b movie go really think television series much better job overall special effect character portrayal let hope producer cast get next series crusade standard b']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Word2Vec Embeddings + Logistic Regression\n",
        "print(\"Training Word2Vec model...\")\n",
        "word2vec_model = Word2Vec([x.split() for x in X_train], vector_size=100, window=5, min_count=2, workers=4)\n",
        "word2vec_vectors = word2vec_model.wv\n",
        "\n",
        "def get_word2vec_embedding(text):\n",
        "    tokens = text.split()\n",
        "    return np.mean([word2vec_vectors[word] for word in tokens if word in word2vec_vectors], axis=0)\n",
        "\n",
        "X_train_word2vec = np.array([get_word2vec_embedding(text) for text in X_train])\n",
        "X_test_word2vec = np.array([get_word2vec_embedding(text) for text in X_test])\n",
        "\n",
        "# Logistic Regression on Word2Vec embeddings\n",
        "clf_word2vec = LogisticRegression(max_iter=1000)\n",
        "clf_word2vec.fit(X_train_word2vec, y_train)\n",
        "\n",
        "precision, recall, f1 = evaluate_model(clf_word2vec, X_test_word2vec, y_test)\n",
        "new_row = pd.DataFrame({\"Model\": [\"Word2Vec + Logistic Regression\"], \"Precision\": [precision], \"Recall\": [recall], \"F1 Score\": [f1]})\n",
        "results_table = pd.concat([results_table, new_row], ignore_index=True)\n",
        "\n",
        "## 2. FastText Embeddings + Logistic Regression\n",
        "print(\"Training FastText model...\")\n",
        "fasttext_model = FastText([x.split() for x in X_train], vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "def get_fasttext_embedding(text):\n",
        "    tokens = text.split()\n",
        "    return np.mean([fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv], axis=0)\n",
        "\n",
        "X_train_fasttext = np.array([get_fasttext_embedding(text) for text in X_train])\n",
        "X_test_fasttext = np.array([get_fasttext_embedding(text) for text in X_test])\n",
        "\n",
        "# Logistic Regression on FastText embeddings\n",
        "clf_fasttext = LogisticRegression(max_iter=1000)\n",
        "clf_fasttext.fit(X_train_fasttext, y_train)\n",
        "\n",
        "precision, recall, f1 = evaluate_model(clf_fasttext, X_test_fasttext, y_test)\n",
        "new_row = pd.DataFrame({\"Model\": [\"FastText + Logistic Regression\"], \"Precision\": [precision], \"Recall\": [recall], \"F1 Score\": [f1]})\n",
        "results_table = pd.concat([results_table, new_row], ignore_index=True)\n",
        "\n",
        "## 3. GloVe Embeddings + Logistic Regression\n",
        "\n",
        "# Path to GloVe file on Google Drive\n",
        "glove_path = '/content/drive/My Drive/glove.6B.100d.txt'\n",
        "\n",
        "# Step 1: Count the number of lines (words) in the GloVe file\n",
        "def count_lines(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return sum(1 for _ in f)\n",
        "\n",
        "# Step 2: Preallocate KeyedVectors\n",
        "num_words = count_lines(glove_path)\n",
        "vector_size = 100  # GloVe 100d has 100-dimensional vectors\n",
        "\n",
        "# Preallocate KeyedVectors with the number of words and vector size\n",
        "glove_vectors = KeyedVectors(vector_size=vector_size, count=num_words)\n",
        "\n",
        "# Step 3: Load GloVe vectors with progress bar\n",
        "print(\"Loading GloVe embeddings...\")\n",
        "\n",
        "# Using tqdm to show the progress bar while loading the GloVe file\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    batch = []\n",
        "    for line in tqdm(f, total=num_words, desc=\"Processing GloVe Vectors\"):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        batch.append((word, vector))\n",
        "\n",
        "    glove_vectors.add_vectors([word for word, vector in batch], np.array([vector for word, vector in batch]))\n",
        "\n",
        "# Normalize vectors\n",
        "glove_vectors.fill_norms()\n",
        "\n",
        "# Function to get GloVe embeddings for a given text\n",
        "def get_glove_embedding(text):\n",
        "    tokens = text.split()\n",
        "    vectors = [glove_vectors[word] for word in tokens if word in glove_vectors]\n",
        "\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Return a zero vector if no valid words are found\n",
        "\n",
        "X_train_glove = np.array([get_glove_embedding(text) for text in tqdm(X_train, desc=\"Processing Training Set\")])\n",
        "X_test_glove = np.array([get_glove_embedding(text) for text in tqdm(X_test, desc=\"Processing Test Set\")])\n",
        "\n",
        "# Logistic Regression on GloVe embeddings\n",
        "clf_glove = LogisticRegression(max_iter=1000)\n",
        "clf_glove.fit(X_train_glove, y_train)\n",
        "\n",
        "precision, recall, f1 = evaluate_model(clf_glove, X_test_glove, y_test)\n",
        "new_row = pd.DataFrame({\"Model\": [\"GloVe + Logistic Regression\"], \"Precision\": [precision], \"Recall\": [recall], \"F1 Score\": [f1]})\n",
        "results_table = pd.concat([results_table, new_row], ignore_index=True)\n",
        "\n",
        "# Show final results table\n",
        "print(results_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s85of0zAgkv",
        "outputId": "fe1d9981-58ec-4edb-a914-8d2521a13657"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-e1f230d2c137>:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_table = pd.concat([results_table, new_row], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FastText model...\n",
            "Loading GloVe embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing GloVe Vectors: 100%|██████████| 400001/400001 [00:37<00:00, 10548.13it/s]\n",
            "Processing Training Set: 100%|██████████| 20000/20000 [00:28<00:00, 709.31it/s] \n",
            "Processing Test Set: 100%|██████████| 5000/5000 [00:08<00:00, 565.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            Model  Precision    Recall  F1 Score\n",
            "0  Word2Vec + Logistic Regression   0.824602  0.854443  0.839258\n",
            "1  FastText + Logistic Regression   0.811866  0.836349  0.823926\n",
            "2     GloVe + Logistic Regression   0.778750  0.786892  0.782800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Word2Vec Embeddings + SVM\n",
        "print(\"Training Word2Vec model...\")\n",
        "word2vec_model = Word2Vec([x.split() for x in X_train], vector_size=100, window=5, min_count=2, workers=4)\n",
        "word2vec_vectors = word2vec_model.wv\n",
        "\n",
        "def get_word2vec_embedding(text):\n",
        "    tokens = text.split()\n",
        "    return np.mean([word2vec_vectors[word] for word in tokens if word in word2vec_vectors], axis=0)\n",
        "\n",
        "X_train_word2vec = np.array([get_word2vec_embedding(text) for text in X_train])\n",
        "X_test_word2vec = np.array([get_word2vec_embedding(text) for text in X_test])\n",
        "\n",
        "# SVM on Word2Vec embeddings\n",
        "clf_word2vec = SVC(kernel='linear', max_iter=1000)  # Używamy SVM z liniowym jądrem\n",
        "clf_word2vec.fit(X_train_word2vec, y_train)\n",
        "\n",
        "precision, recall, f1 = evaluate_model(clf_word2vec, X_test_word2vec, y_test)\n",
        "new_row = pd.DataFrame({\"Model\": [\"Word2Vec + SVM\"], \"Precision\": [precision], \"Recall\": [recall], \"F1 Score\": [f1]})\n",
        "results_table = pd.concat([results_table, new_row], ignore_index=True)\n",
        "\n",
        "## 2. FastText Embeddings + SVM\n",
        "print(\"Training FastText model...\")\n",
        "fasttext_model = FastText([x.split() for x in X_train], vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "def get_fasttext_embedding(text):\n",
        "    tokens = text.split()\n",
        "    return np.mean([fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv], axis=0)\n",
        "\n",
        "X_train_fasttext = np.array([get_fasttext_embedding(text) for text in X_train])\n",
        "X_test_fasttext = np.array([get_fasttext_embedding(text) for text in X_test])\n",
        "\n",
        "# SVM on FastText embeddings\n",
        "clf_fasttext = SVC(kernel='linear', max_iter=1000)  # Używamy SVM z liniowym jądrem\n",
        "clf_fasttext.fit(X_train_fasttext, y_train)\n",
        "\n",
        "precision, recall, f1 = evaluate_model(clf_fasttext, X_test_fasttext, y_test)\n",
        "new_row = pd.DataFrame({\"Model\": [\"FastText + SVM\"], \"Precision\": [precision], \"Recall\": [recall], \"F1 Score\": [f1]})\n",
        "results_table = pd.concat([results_table, new_row], ignore_index=True)\n",
        "\n",
        "## 3. GloVe Embeddings + SVM\n",
        "\n",
        "# Path to GloVe file on Google Drive\n",
        "glove_path = '/content/drive/My Drive/glove.6B.100d.txt'\n",
        "\n",
        "# Step 1: Count the number of lines (words) in the GloVe file\n",
        "def count_lines(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return sum(1 for _ in f)\n",
        "\n",
        "# Step 2: Preallocate KeyedVectors\n",
        "num_words = count_lines(glove_path)\n",
        "vector_size = 100  # GloVe 100d has 100-dimensional vectors\n",
        "\n",
        "# Preallocate KeyedVectors with the number of words and vector size\n",
        "glove_vectors = KeyedVectors(vector_size=vector_size, count=num_words)\n",
        "\n",
        "# Step 3: Load GloVe vectors with progress bar\n",
        "print(\"Loading GloVe embeddings...\")\n",
        "\n",
        "# Using tqdm to show the progress bar while loading the GloVe file\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    batch = []\n",
        "    for line in tqdm(f, total=num_words, desc=\"Processing GloVe Vectors\"):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        batch.append((word, vector))\n",
        "\n",
        "    glove_vectors.add_vectors([word for word, vector in batch], np.array([vector for word, vector in batch]))\n",
        "\n",
        "# Normalize vectors\n",
        "glove_vectors.fill_norms()\n",
        "\n",
        "# Function to get GloVe embeddings for a given text\n",
        "def get_glove_embedding(text):\n",
        "    tokens = text.split()\n",
        "    vectors = [glove_vectors[word] for word in tokens if word in glove_vectors]\n",
        "\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Return a zero vector if no valid words are found\n",
        "\n",
        "X_train_glove = np.array([get_glove_embedding(text) for text in tqdm(X_train, desc=\"Processing Training Set\")])\n",
        "X_test_glove = np.array([get_glove_embedding(text) for text in tqdm(X_test, desc=\"Processing Test Set\")])\n",
        "\n",
        "# SVM on GloVe embeddings\n",
        "clf_glove = SVC(kernel='linear', max_iter=1000)  # Używamy SVM z liniowym jądrem\n",
        "clf_glove.fit(X_train_glove, y_train)\n",
        "\n",
        "precision, recall, f1 = evaluate_model(clf_glove, X_test_glove, y_test)\n",
        "new_row = pd.DataFrame({\"Model\": [\"GloVe + SVM\"], \"Precision\": [precision], \"Recall\": [recall], \"F1 Score\": [f1]})\n",
        "results_table = pd.concat([results_table, new_row], ignore_index=True)\n",
        "\n",
        "# Show final results table\n",
        "print(results_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJAz2P7fB93a",
        "outputId": "4d9a7b97-43a4-46c8-90f4-96e5438d1d93"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n",
            "<ipython-input-56-34e7159b5cfa>:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results_table = pd.concat([results_table, new_row], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FastText model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing GloVe Vectors: 100%|██████████| 400001/400001 [00:39<00:00, 10169.42it/s]\n",
            "Processing Training Set: 100%|██████████| 20000/20000 [00:33<00:00, 590.63it/s]\n",
            "Processing Test Set: 100%|██████████| 5000/5000 [00:07<00:00, 703.82it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Model  Precision    Recall  F1 Score\n",
            "0  Word2Vec + SVM   0.523464  0.988719  0.684519\n",
            "1  FastText + SVM   0.514673  0.932716  0.663324\n",
            "2     GloVe + SVM   0.496698  1.000000  0.663725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The best F1 score -> Word2Vec + Logistic Regression -> 0.839258"
      ],
      "metadata": {
        "id": "YQ8pZbSkPbJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}