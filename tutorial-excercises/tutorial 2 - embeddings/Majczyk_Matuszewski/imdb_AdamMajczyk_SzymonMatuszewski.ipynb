{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Data loading\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "df_train = pd.DataFrame(ds['train'])\n",
    "df_test = pd.DataFrame(ds['test'])\n",
    "\n",
    "# Data preprocessing\n",
    "X_train = df_train['text']\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_test = df_test['text']\n",
    "y_test = df_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    I rented I AM CURIOUS-YELLOW from my video sto...\n",
       " 1    \"I Am Curious: Yellow\" is a risible and preten...\n",
       " 2    If only to avoid making this type of film in t...\n",
       " 3    This film was probably inspired by Godard's Ma...\n",
       " 4    Oh, brother...after hearing about this ridicul...\n",
       " Name: text, dtype: object,\n",
       " 0    0\n",
       " 1    0\n",
       " 2    0\n",
       " 3    0\n",
       " 4    0\n",
       " Name: label, dtype: int64,\n",
       " 0    I love sci-fi and am willing to put up with a ...\n",
       " 1    Worth the entertainment value of a rental, esp...\n",
       " 2    its a totally average film with a few semi-alr...\n",
       " 3    STAR RATING: ***** Saturday Night **** Friday ...\n",
       " 4    First off let me say, If you haven't enjoyed a...\n",
       " Name: text, dtype: object,\n",
       " 0    0\n",
       " 1    0\n",
       " 2    0\n",
       " 3    0\n",
       " 4    0\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(), y_train.head(), X_test.head(), y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "max_features = [1000, 5000]\n",
    "ngram_range = [(1,1), (1,2), (1,3)]\n",
    "\n",
    "tfidfs = []\n",
    "\n",
    "for max_feature in max_features:\n",
    "    for ngram in ngram_range:\n",
    "        tfidf = TfidfVectorizer(max_features=max_feature, ngram_range=ngram)\n",
    "        X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "        X_test_tfidf = tfidf.transform(X_test)\n",
    "        tfid_dict = {\n",
    "            'max_feature': max_feature,\n",
    "            'ngram': ngram,\n",
    "            'X_train_tfidf': X_train_tfidf,\n",
    "            'X_test_tfidf': X_test_tfidf\n",
    "        }\n",
    "        tfidfs.append(tfid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec  \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_word2vec(tokens, model, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vec += model.wv[word].reshape((1, size))\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Tokenization\n",
    "X_train_tokens = [text.split() for text in X_train]\n",
    "X_test_tokens = [text.split() for text in X_test]\n",
    "\n",
    "# Model Word2Vec\n",
    "vector_sizes = [50, 100]\n",
    "windows = [5, 10]\n",
    "min_counts = [1, 2]\n",
    "\n",
    "w2v_models = []\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=vector_size, window=window, min_count=min_count, workers=4)\n",
    "            X_train_w2v = np.array([get_average_word2vec(tokens, w2v_model, vector_size) for tokens in X_train_tokens])\n",
    "            X_test_w2v = np.array([get_average_word2vec(tokens, w2v_model, vector_size) for tokens in X_test_tokens])\n",
    "            w2v_dict = {\n",
    "                'vector_size': vector_size,\n",
    "                'window': window,\n",
    "                'min_count': min_count,\n",
    "                'X_train_w2v': X_train_w2v,\n",
    "                'X_test_w2v': X_test_w2v\n",
    "            }\n",
    "            w2v_models.append(w2v_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Model FastText\n",
    "fasttext_model = FastText(sentences=X_train_tokens, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Uśrednianie wektorów dla FastText\n",
    "X_train_fasttext = np.array([get_average_word2vec(tokens, fasttext_model, 100) for tokens in X_train_tokens])\n",
    "X_test_fasttext = np.array([get_average_word2vec(tokens, fasttext_model, 100) for tokens in X_test_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttexts = [{\n",
    "    'X_train_fasttext': X_train_fasttext,\n",
    "    'X_test_fasttext': X_test_fasttext\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "param_grid_lgbm = {\n",
    "    'num_leaves': [31, 50],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "}\n",
    "\n",
    "param_grid_catboost = {\n",
    "    'iterations': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'depth': [3, 5],\n",
    "}\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'model': LGBMClassifier(),\n",
    "        'param_grid': param_grid_lgbm\n",
    "    },\n",
    "    {\n",
    "        'model': xgb.XGBClassifier(),\n",
    "        'param_grid': param_grid_xgb\n",
    "    },\n",
    "    {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'param_grid': param_grid_rf\n",
    "    },\n",
    "    {\n",
    "        'model': CatBoostClassifier(),\n",
    "        'param_grid': param_grid_catboost\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttexts[0]['X_train_fasttext'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [08:09<00:00, 489.21s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:59<00:00, 59.87s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [01:50<00:00, 110.65s/it]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [24:35<00:00, 1475.71s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [07:34<00:00, 454.61s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [14:38<00:00, 878.61s/it]\n",
      "\n",
      "100%|██████████| 2/2 [57:48<00:00, 1734.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "df_scores = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "\n",
    "for model_dict in tqdm(models[:2]):\n",
    "    for tfidf_dict in tqdm(tfidfs[:1]):\n",
    "        model = model_dict['model']\n",
    "        param_grid = model_dict['param_grid']\n",
    "        X_train_tfidf = tfidf_dict['X_train_tfidf']\n",
    "        X_test_tfidf = tfidf_dict['X_test_tfidf']\n",
    "        model_name = model.__class__.__name__ + ' ' + str(tfidf_dict['max_feature']) + ' ' + str(tfidf_dict['ngram'])\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(X_train_tfidf, y_train)\n",
    "        y_pred = grid_search.predict(X_test_tfidf)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame([{\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }])], ignore_index=True)\n",
    "        df_scores.to_csv('scores_non_transformer.csv', index=False)\n",
    "        \n",
    "    for w2v_dict in tqdm(w2v_models[:1]):\n",
    "        model = model_dict['model']\n",
    "        param_grid = model_dict['param_grid']\n",
    "        X_train_w2v = w2v_dict['X_train_w2v']\n",
    "        X_train_w2v = X_train_w2v.reshape(-1, 50)\n",
    "        X_test_w2v = w2v_dict['X_test_w2v']\n",
    "        X_test_w2v = X_test_w2v.reshape(-1, 50)\n",
    "        model_name = model.__class__.__name__ + ' ' + str(w2v_dict['vector_size']) + ' ' + str(w2v_dict['window']) + ' ' + str(w2v_dict['min_count'])\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(X_train_w2v, y_train)\n",
    "        y_pred = grid_search.predict(X_test_w2v)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame([{\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }])], ignore_index=True)\n",
    "        df_scores.to_csv('scores_non_transformer.csv', index=False)\n",
    "        \n",
    "    for fasttext_dict in tqdm(fasttexts[:1]):\n",
    "        model = model_dict['model']\n",
    "        param_grid = model_dict['param_grid']\n",
    "        X_train_fasttext = fasttext_dict['X_train_fasttext']\n",
    "        X_train_fasttext = X_train_fasttext.reshape(-1, 100)\n",
    "        X_test_fasttext = fasttext_dict['X_test_fasttext']\n",
    "        X_test_fasttext = X_test_fasttext.reshape(-1, 100)\n",
    "        model_name = model.__class__.__name__ + ' FastText'\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(X_train_fasttext, y_train)\n",
    "        y_pred = grid_search.predict(X_test_fasttext)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        df_scores = pd.concat([df_scores, pd.DataFrame([{\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }])], ignore_index=True)\n",
    "        df_scores.to_csv('scores_non_transformer.csv', index=False)\n",
    "\n",
    "df_scores.to_csv('scores_non_transformer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
