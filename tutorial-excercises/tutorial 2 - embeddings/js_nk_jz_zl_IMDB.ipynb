{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis\n",
    "\n",
    "## Authors\n",
    "1. Jakub Swistak\n",
    "2. Nikita Kozlov\n",
    "3. Jacek Zalewski\n",
    "4. Zosia Lagiewka\n",
    "\n",
    "## Dataset\n",
    "We are using the IMDB dataset with a defined split into train/test, which can be found [here](https://huggingface.co/datasets/stanfordnlp/imdb).\n",
    "\n",
    "## Methods\n",
    "We will try different methods with embedding-based models.\n",
    "## Outcome\n",
    "The outcome will be a metrics for all tested models and data-processing pipelines.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we will perform sentiment analysis on the IMDB dataset using various embedding-based models. The goal is to compare the performance of different models and data-processing pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:56:24.039726Z",
     "start_time": "2024-10-10T14:56:12.920955Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install llmware numpy pandas seaborn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:35:34.254052Z",
     "start_time": "2024-10-10T14:35:19.973693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jakubswistak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jakubswistak/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load iMDB dataset \n",
    "#!%pip install transformers datasets torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from llmware.models import ModelCatalog\n",
    "from gensim.models import FastText, Word2Vec\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "imdb_dataset = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:35:40.226822Z",
     "start_time": "2024-10-10T14:35:40.219471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:36:49.609715Z",
     "start_time": "2024-10-10T14:36:49.601352Z"
    }
   },
   "outputs": [],
   "source": [
    "model_scores = pd.DataFrame(columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:38:27.246425Z",
     "start_time": "2024-10-10T14:38:11.864806Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/9hx8q7517rzf6x75cfny7ndc0000gp/T/ipykernel_18203/295765853.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_scores = pd.concat([model_scores, pd.DataFrame([[\"TextBlob\", f1_textblob, accuracy_textblob, precision_textblob, recall_textblob]], columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.68516</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.94552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model        f1  accuracy  precision   recall\n",
       "0  TextBlob  0.750198   0.68516   0.621758  0.94552"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    return sentiment\n",
    "\n",
    "# Convert list to pandas Series to use apply method\n",
    "imdb_dataset['sentiment_blob'] = imdb_dataset['text'].apply(get_sentiment)\n",
    "f1_textblob = f1_score(imdb_dataset['label'], imdb_dataset['sentiment_blob'].apply(lambda x: 1 if x > 0 else 0))\n",
    "accuracy_textblob = accuracy_score(imdb_dataset['label'], imdb_dataset['sentiment_blob'].apply(lambda x: 1 if x > 0 else 0))\n",
    "precision_textblob = precision_score(imdb_dataset['label'], imdb_dataset['sentiment_blob'].apply(lambda x: 1 if x > 0 else 0))\n",
    "recall_textblob = recall_score(imdb_dataset['label'], imdb_dataset['sentiment_blob'].apply(lambda x: 1 if x > 0 else 0))\n",
    "\n",
    "model_scores = pd.concat([model_scores, pd.DataFrame([[\"TextBlob\", f1_textblob, accuracy_textblob, precision_textblob, recall_textblob]], columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])])\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "total = len(imdb_dataset)\n",
    "\n",
    "current = 0\n",
    "\n",
    "def get_bert_sentiment(text):\n",
    "    global current\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    \n",
    "    print(f\"[{current}/{total}] {text[:10]} -> {model.config.id2label[predicted_class_id]}\")\n",
    "    return model.config.id2label[predicted_class_id]\n",
    "\n",
    "imdb_dataset['sentiment_bert'] = imdb_dataset['text'].apply(get_bert_sentiment).map({'NEGATIVE': 0, 'POSITIVE': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:46:29.976539Z",
     "start_time": "2024-10-10T14:46:29.944196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.68516</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.94552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.88852</td>\n",
       "      <td>0.916117</td>\n",
       "      <td>0.85536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model        f1  accuracy  \\\n",
       "0                                         TextBlob  0.750198   0.68516   \n",
       "0  distilbert-base-uncased-finetuned-sst-2-english  0.884697   0.88852   \n",
       "\n",
       "   precision   recall  \n",
       "0   0.621758  0.94552  \n",
       "0   0.916117  0.85536  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_bert = f1_score(imdb_dataset['label'], imdb_dataset['sentiment_bert'])\n",
    "accuracy_bert = accuracy_score(imdb_dataset['label'], imdb_dataset['sentiment_bert'])\n",
    "precision_bert = precision_score(imdb_dataset['label'], imdb_dataset['sentiment_bert'])\n",
    "recall_bert = recall_score(imdb_dataset['label'], imdb_dataset['sentiment_bert'])\n",
    "\n",
    "model_scores = pd.concat([model_scores, pd.DataFrame([[\"distilbert-base-uncased-finetuned-sst-2-english\", f1_bert, accuracy_bert, precision_bert, recall_bert]], columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])])\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the dataset since all of the samles are quite long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "train, test = train_test_split(imdb_dataset, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slim sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mWARNING: remediation - could not find key-value to correct - output may be missing certain content in structured output.\u001b[39m\n",
      "\u001b[37mINFO: update: function call output could not be automatically converted, but remediation was successful to type - dict \u001b[39m\n",
      "\u001b[37mWARNING: remediation - could not find key-value to correct - output may be missing certain content in structured output.\u001b[39m\n",
      "\u001b[37mWARNING: remediation - could not find key-value to correct - output may be missing certain content in structured output.\u001b[39m\n",
      "\u001b[37mINFO: update: function call output could not be automatically converted, but remediation was successful to type - dict \u001b[39m\n"
     ]
    }
   ],
   "source": [
    "slim_model = ModelCatalog().load_model(\"llmware/slim-sentiment\")\n",
    "\n",
    "def get_sentiment_llm(text):\n",
    "    response = slim_model.function_call(text, params=[\"sentiment\"], function=\"classify\")\n",
    "    return response\n",
    "\n",
    "test['sentiment_slim_unprocessed'] = test['text'].apply(get_sentiment_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:47:26.392874Z",
     "start_time": "2024-10-10T14:47:26.308851Z"
    }
   },
   "outputs": [],
   "source": [
    "test[\"sentiment_slim_processed\"] = test[\"sentiment_slim_unprocessed\"].apply(lambda x: x['llm_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:47:35.924557Z",
     "start_time": "2024-10-10T14:47:35.290039Z"
    }
   },
   "outputs": [],
   "source": [
    "test[\"sentiment_slim\"] = test[\"sentiment_slim_processed\"].apply(lambda x: 1 if x.get('sentiment', ['negative'])[0] == \"positive\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T14:48:45.714541Z",
     "start_time": "2024-10-10T14:48:45.701707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.68516</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.945520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.88852</td>\n",
       "      <td>0.916117</td>\n",
       "      <td>0.855360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slim-sentiment</td>\n",
       "      <td>0.901526</td>\n",
       "      <td>0.90060</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model        f1  accuracy  \\\n",
       "0                                         TextBlob  0.750198   0.68516   \n",
       "0  distilbert-base-uncased-finetuned-sst-2-english  0.884697   0.88852   \n",
       "0                                   slim-sentiment  0.901526   0.90060   \n",
       "\n",
       "   precision    recall  \n",
       "0   0.621758  0.945520  \n",
       "0   0.916117  0.855360  \n",
       "0   0.887978  0.915493  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_slim = f1_score(test['label'], test['sentiment_slim'])\n",
    "accuracy_slim = accuracy_score(test['label'], test['sentiment_slim'])\n",
    "precision_slim = precision_score(test['label'], test['sentiment_slim'])\n",
    "recall_slim = recall_score(test['label'], test['sentiment_slim'])\n",
    "\n",
    "model_scores = pd.concat([model_scores, pd.DataFrame([[\"slim-sentiment\", f1_slim, accuracy_slim, precision_slim, recall_slim]], columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])])\n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T15:04:30.506331Z",
     "start_time": "2024-10-10T15:04:09.276899Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nk2/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def get_sentiment_vader(text):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(text)['compound']\n",
    "\n",
    "test['sentiment_vader'] = test['text'].apply(get_sentiment_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T15:04:42.034628Z",
     "start_time": "2024-10-10T15:04:42.005698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.68516</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.945520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.88852</td>\n",
       "      <td>0.916117</td>\n",
       "      <td>0.855360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slim-sentiment</td>\n",
       "      <td>0.901526</td>\n",
       "      <td>0.90060</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vader</td>\n",
       "      <td>0.733529</td>\n",
       "      <td>0.69180</td>\n",
       "      <td>0.643117</td>\n",
       "      <td>0.853521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model        f1  accuracy  \\\n",
       "0                                         TextBlob  0.750198   0.68516   \n",
       "0  distilbert-base-uncased-finetuned-sst-2-english  0.884697   0.88852   \n",
       "0                                   slim-sentiment  0.901526   0.90060   \n",
       "0                                            Vader  0.733529   0.69180   \n",
       "\n",
       "   precision    recall  \n",
       "0   0.621758  0.945520  \n",
       "0   0.916117  0.855360  \n",
       "0   0.887978  0.915493  \n",
       "0   0.643117  0.853521  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_vader = f1_score(test['label'], test['sentiment_vader'].apply(lambda x: 1 if x > 0 else 0))\n",
    "accuracy_vader = accuracy_score(test['label'], test['sentiment_vader'].apply(lambda x: 1 if x > 0 else 0))\n",
    "precision_vader = precision_score(test['label'], test['sentiment_vader'].apply(lambda x: 1 if x > 0 else 0))\n",
    "recall_vader = recall_score(test['label'], test['sentiment_vader'].apply(lambda x: 1 if x > 0 else 0))\n",
    "\n",
    "model_scores = pd.concat([model_scores, pd.DataFrame([[\"Vader\", f1_vader, accuracy_vader, precision_vader, recall_vader]], columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])])\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [00:59:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.68516</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.945520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.88852</td>\n",
       "      <td>0.916117</td>\n",
       "      <td>0.855360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slim-sentiment</td>\n",
       "      <td>0.901526</td>\n",
       "      <td>0.90060</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vader</td>\n",
       "      <td>0.733529</td>\n",
       "      <td>0.69180</td>\n",
       "      <td>0.643117</td>\n",
       "      <td>0.853521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec + Logistic Regression</td>\n",
       "      <td>0.815889</td>\n",
       "      <td>0.81460</td>\n",
       "      <td>0.805490</td>\n",
       "      <td>0.826559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec + AdaBoost</td>\n",
       "      <td>0.754498</td>\n",
       "      <td>0.75440</td>\n",
       "      <td>0.749702</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec + SVM</td>\n",
       "      <td>0.809609</td>\n",
       "      <td>0.80820</td>\n",
       "      <td>0.798981</td>\n",
       "      <td>0.820523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec + Random Forest</td>\n",
       "      <td>0.763702</td>\n",
       "      <td>0.76460</td>\n",
       "      <td>0.762019</td>\n",
       "      <td>0.765392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec + XGBoost</td>\n",
       "      <td>0.797777</td>\n",
       "      <td>0.79620</td>\n",
       "      <td>0.787001</td>\n",
       "      <td>0.808853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             model        f1  accuracy  \\\n",
       "0                                         TextBlob  0.750198   0.68516   \n",
       "0  distilbert-base-uncased-finetuned-sst-2-english  0.884697   0.88852   \n",
       "0                                   slim-sentiment  0.901526   0.90060   \n",
       "0                                            Vader  0.733529   0.69180   \n",
       "0                   Word2Vec + Logistic Regression  0.815889   0.81460   \n",
       "0                              Word2Vec + AdaBoost  0.754498   0.75440   \n",
       "0                                   Word2Vec + SVM  0.809609   0.80820   \n",
       "0                         Word2Vec + Random Forest  0.763702   0.76460   \n",
       "0                               Word2Vec + XGBoost  0.797777   0.79620   \n",
       "\n",
       "   precision    recall  \n",
       "0   0.621758  0.945520  \n",
       "0   0.916117  0.855360  \n",
       "0   0.887978  0.915493  \n",
       "0   0.643117  0.853521  \n",
       "0   0.805490  0.826559  \n",
       "0   0.749702  0.759356  \n",
       "0   0.798981  0.820523  \n",
       "0   0.762019  0.765392  \n",
       "0   0.787001  0.808853  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "imdb_dataset['nltk_tokens'] = imdb_dataset['text'].apply(tokenize)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(imdb_dataset['nltk_tokens'], imdb_dataset['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def vectorize_sentence(sentence, model):\n",
    "    valid_words = [word for word in sentence if word in model.wv.key_to_index]\n",
    "    if len(valid_words) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_vectors = np.array([vectorize_sentence(sentence, word2vec_model) for sentence in X_train])\n",
    "X_test_vectors = np.array([vectorize_sentence(sentence, word2vec_model) for sentence in X_test])\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_vectors, y_train)\n",
    "    y_pred = classifier.predict(X_test_vectors)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    model_scores = pd.concat([model_scores, pd.DataFrame([[f\"Word2Vec + {model_name}\", f1, accuracy, precision, recall]], columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])])\n",
    "model_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [01:02:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.68516</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.945520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.88852</td>\n",
       "      <td>0.916117</td>\n",
       "      <td>0.855360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slim-sentiment</td>\n",
       "      <td>0.901526</td>\n",
       "      <td>0.90060</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vader</td>\n",
       "      <td>0.733529</td>\n",
       "      <td>0.69180</td>\n",
       "      <td>0.643117</td>\n",
       "      <td>0.853521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word2Vec + Logistic Regression</td>\n",
       "      <td>0.815889</td>\n",
       "      <td>0.81460</td>\n",
       "      <td>0.805490</td>\n",
       "      <td>0.826559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word2Vec + AdaBoost</td>\n",
       "      <td>0.754498</td>\n",
       "      <td>0.75440</td>\n",
       "      <td>0.749702</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2Vec + SVM</td>\n",
       "      <td>0.809609</td>\n",
       "      <td>0.80820</td>\n",
       "      <td>0.798981</td>\n",
       "      <td>0.820523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Word2Vec + Random Forest</td>\n",
       "      <td>0.763702</td>\n",
       "      <td>0.76460</td>\n",
       "      <td>0.762019</td>\n",
       "      <td>0.765392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Word2Vec + XGBoost</td>\n",
       "      <td>0.797777</td>\n",
       "      <td>0.79620</td>\n",
       "      <td>0.787001</td>\n",
       "      <td>0.808853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FastText + Logistic Regression</td>\n",
       "      <td>0.799682</td>\n",
       "      <td>0.79840</td>\n",
       "      <td>0.789949</td>\n",
       "      <td>0.809658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FastText + AdaBoost</td>\n",
       "      <td>0.694584</td>\n",
       "      <td>0.69660</td>\n",
       "      <td>0.695004</td>\n",
       "      <td>0.694165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FastText + SVM</td>\n",
       "      <td>0.793145</td>\n",
       "      <td>0.79480</td>\n",
       "      <td>0.794747</td>\n",
       "      <td>0.791549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FastText + Random Forest</td>\n",
       "      <td>0.701532</td>\n",
       "      <td>0.70780</td>\n",
       "      <td>0.712448</td>\n",
       "      <td>0.690946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FastText + XGBoost</td>\n",
       "      <td>0.732929</td>\n",
       "      <td>0.73560</td>\n",
       "      <td>0.735903</td>\n",
       "      <td>0.729980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              model        f1  accuracy  \\\n",
       "0                                          TextBlob  0.750198   0.68516   \n",
       "1   distilbert-base-uncased-finetuned-sst-2-english  0.884697   0.88852   \n",
       "2                                    slim-sentiment  0.901526   0.90060   \n",
       "3                                             Vader  0.733529   0.69180   \n",
       "4                    Word2Vec + Logistic Regression  0.815889   0.81460   \n",
       "5                               Word2Vec + AdaBoost  0.754498   0.75440   \n",
       "6                                    Word2Vec + SVM  0.809609   0.80820   \n",
       "7                          Word2Vec + Random Forest  0.763702   0.76460   \n",
       "8                                Word2Vec + XGBoost  0.797777   0.79620   \n",
       "9                    FastText + Logistic Regression  0.799682   0.79840   \n",
       "10                              FastText + AdaBoost  0.694584   0.69660   \n",
       "11                                   FastText + SVM  0.793145   0.79480   \n",
       "12                         FastText + Random Forest  0.701532   0.70780   \n",
       "13                               FastText + XGBoost  0.732929   0.73560   \n",
       "\n",
       "    precision    recall  \n",
       "0    0.621758  0.945520  \n",
       "1    0.916117  0.855360  \n",
       "2    0.887978  0.915493  \n",
       "3    0.643117  0.853521  \n",
       "4    0.805490  0.826559  \n",
       "5    0.749702  0.759356  \n",
       "6    0.798981  0.820523  \n",
       "7    0.762019  0.765392  \n",
       "8    0.787001  0.808853  \n",
       "9    0.789949  0.809658  \n",
       "10   0.695004  0.694165  \n",
       "11   0.794747  0.791549  \n",
       "12   0.712448  0.690946  \n",
       "13   0.735903  0.729980  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def vectorize_sentence(sentence, model):\n",
    "    valid_words = [word for word in sentence if word in model.wv.key_to_index]\n",
    "    if len(valid_words) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_vectors = np.array([vectorize_sentence(sentence, fasttext_model) for sentence in X_train])\n",
    "X_test_vectors = np.array([vectorize_sentence(sentence, fasttext_model) for sentence in X_test])\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_vectors, y_train)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test_vectors)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    model_scores = pd.concat([model_scores, pd.DataFrame([[f\"FastText + {model_name}\", f1, accuracy, precision, recall]], \n",
    "                                                         columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])], ignore_index=True)\n",
    "    \n",
    "\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2195892 embeddings from glove840B300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [02:38:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.68516</td>\n",
       "      <td>0.621758</td>\n",
       "      <td>0.945520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>0.884697</td>\n",
       "      <td>0.88852</td>\n",
       "      <td>0.916117</td>\n",
       "      <td>0.855360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slim-sentiment</td>\n",
       "      <td>0.901526</td>\n",
       "      <td>0.90060</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vader</td>\n",
       "      <td>0.733529</td>\n",
       "      <td>0.69180</td>\n",
       "      <td>0.643117</td>\n",
       "      <td>0.853521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Word2Vec + Logistic Regression</td>\n",
       "      <td>0.815889</td>\n",
       "      <td>0.81460</td>\n",
       "      <td>0.805490</td>\n",
       "      <td>0.826559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word2Vec + AdaBoost</td>\n",
       "      <td>0.754498</td>\n",
       "      <td>0.75440</td>\n",
       "      <td>0.749702</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Word2Vec + SVM</td>\n",
       "      <td>0.809609</td>\n",
       "      <td>0.80820</td>\n",
       "      <td>0.798981</td>\n",
       "      <td>0.820523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Word2Vec + Random Forest</td>\n",
       "      <td>0.763702</td>\n",
       "      <td>0.76460</td>\n",
       "      <td>0.762019</td>\n",
       "      <td>0.765392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Word2Vec + XGBoost</td>\n",
       "      <td>0.797777</td>\n",
       "      <td>0.79620</td>\n",
       "      <td>0.787001</td>\n",
       "      <td>0.808853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FastText + Logistic Regression</td>\n",
       "      <td>0.799682</td>\n",
       "      <td>0.79840</td>\n",
       "      <td>0.789949</td>\n",
       "      <td>0.809658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FastText + AdaBoost</td>\n",
       "      <td>0.694584</td>\n",
       "      <td>0.69660</td>\n",
       "      <td>0.695004</td>\n",
       "      <td>0.694165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FastText + SVM</td>\n",
       "      <td>0.793145</td>\n",
       "      <td>0.79480</td>\n",
       "      <td>0.794747</td>\n",
       "      <td>0.791549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FastText + Random Forest</td>\n",
       "      <td>0.701532</td>\n",
       "      <td>0.70780</td>\n",
       "      <td>0.712448</td>\n",
       "      <td>0.690946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FastText + XGBoost</td>\n",
       "      <td>0.732929</td>\n",
       "      <td>0.73560</td>\n",
       "      <td>0.735903</td>\n",
       "      <td>0.729980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GloVe + Logistic Regression</td>\n",
       "      <td>0.852835</td>\n",
       "      <td>0.85360</td>\n",
       "      <td>0.852149</td>\n",
       "      <td>0.853521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GloVe + AdaBoost</td>\n",
       "      <td>0.795413</td>\n",
       "      <td>0.79660</td>\n",
       "      <td>0.795253</td>\n",
       "      <td>0.795573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GloVe + SVM</td>\n",
       "      <td>0.844274</td>\n",
       "      <td>0.84580</td>\n",
       "      <td>0.847526</td>\n",
       "      <td>0.841046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GloVe + Random Forest</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.80240</td>\n",
       "      <td>0.804888</td>\n",
       "      <td>0.795171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GloVe + XGBoost</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.83600</td>\n",
       "      <td>0.831014</td>\n",
       "      <td>0.841046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              model        f1  accuracy  \\\n",
       "0                                          TextBlob  0.750198   0.68516   \n",
       "1   distilbert-base-uncased-finetuned-sst-2-english  0.884697   0.88852   \n",
       "2                                    slim-sentiment  0.901526   0.90060   \n",
       "3                                             Vader  0.733529   0.69180   \n",
       "4                    Word2Vec + Logistic Regression  0.815889   0.81460   \n",
       "5                               Word2Vec + AdaBoost  0.754498   0.75440   \n",
       "6                                    Word2Vec + SVM  0.809609   0.80820   \n",
       "7                          Word2Vec + Random Forest  0.763702   0.76460   \n",
       "8                                Word2Vec + XGBoost  0.797777   0.79620   \n",
       "9                    FastText + Logistic Regression  0.799682   0.79840   \n",
       "10                              FastText + AdaBoost  0.694584   0.69660   \n",
       "11                                   FastText + SVM  0.793145   0.79480   \n",
       "12                         FastText + Random Forest  0.701532   0.70780   \n",
       "13                               FastText + XGBoost  0.732929   0.73560   \n",
       "14                      GloVe + Logistic Regression  0.852835   0.85360   \n",
       "15                                 GloVe + AdaBoost  0.795413   0.79660   \n",
       "16                                      GloVe + SVM  0.844274   0.84580   \n",
       "17                            GloVe + Random Forest  0.800000   0.80240   \n",
       "18                                  GloVe + XGBoost  0.836000   0.83600   \n",
       "\n",
       "    precision    recall  \n",
       "0    0.621758  0.945520  \n",
       "1    0.916117  0.855360  \n",
       "2    0.887978  0.915493  \n",
       "3    0.643117  0.853521  \n",
       "4    0.805490  0.826559  \n",
       "5    0.749702  0.759356  \n",
       "6    0.798981  0.820523  \n",
       "7    0.762019  0.765392  \n",
       "8    0.787001  0.808853  \n",
       "9    0.789949  0.809658  \n",
       "10   0.695004  0.694165  \n",
       "11   0.794747  0.791549  \n",
       "12   0.712448  0.690946  \n",
       "13   0.735903  0.729980  \n",
       "14   0.852149  0.853521  \n",
       "15   0.795253  0.795573  \n",
       "16   0.847526  0.841046  \n",
       "17   0.804888  0.795171  \n",
       "18   0.831014  0.841046  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dimension_size(line):\n",
    "    size = 0\n",
    "    l_split = line.strip().split()\n",
    "    for i in l_split:\n",
    "        try:\n",
    "            _ = float(i)\n",
    "            size = size + 1\n",
    "        except:\n",
    "            pass\n",
    "    return size\n",
    "\n",
    "def get_embeddings(file):\n",
    "    embs = dict()\n",
    "    firstLine = open(file, 'r').readline()\n",
    "    dimension = get_dimension_size(firstLine)\n",
    "    for l in open(file, 'r').readlines():\n",
    "        l_split = l.strip().split()\n",
    "        if len(l_split) == 2:\n",
    "            continue\n",
    "        emb = l_split[-1 * dimension:]\n",
    "        word = l_split[:-1 * dimension]\n",
    "        word = ''.join(word)\n",
    "        embs[word] = [float(em) for em in emb]\n",
    "    print(\"Got {} embeddings from {}\".format(len(embs), file))\n",
    "    return embs\n",
    "\n",
    "glove_file = 'glove840B300d.txt' \n",
    "glove_embeddings = get_embeddings(glove_file)\n",
    "\n",
    "def vectorize_sentence(sentence, embeddings, vector_size=300):\n",
    "    valid_words = [word for word in sentence if word in embeddings]\n",
    "    if len(valid_words) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean([embeddings[word] for word in valid_words], axis=0)\n",
    "\n",
    "X_train_vectors = np.array([vectorize_sentence(sentence, glove_embeddings) for sentence in X_train])\n",
    "X_test_vectors = np.array([vectorize_sentence(sentence, glove_embeddings) for sentence in X_test])\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_vectors, y_train)\n",
    "    y_pred = classifier.predict(X_test_vectors)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    model_scores = pd.concat([model_scores, pd.DataFrame([[f\"GloVe + {model_name}\", f1, accuracy, precision, recall]], \n",
    "                                                         columns=[\"model\", \"f1\", \"accuracy\", \"precision\", \"recall\"])], ignore_index=True)\n",
    "\n",
    "model_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
