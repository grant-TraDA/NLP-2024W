{"cells":[{"cell_type":"markdown","metadata":{"id":"zvZABs451wcg"},"source":["# Tutorial 2 - embeddings\n","\n","Authors: Michał Gromadzki, Kacper Skonieczka\n","\n","TASK: sentiment analysis classification\n","\n","This project performs sentiment analysis on the IMDb movie reviews dataset using a transformer-based model, DistilBERT. DistilBERT is a smaller, faster variant of BERT and is well-suited for text classification tasks like sentiment analysis. We will preprocess the data, train the model, and evaluate its performance using common classification metrics such as F1 Score, Precision, Recall, and Accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:37:48.173102Z","iopub.status.busy":"2024-10-09T15:37:48.172698Z","iopub.status.idle":"2024-10-09T15:37:51.083702Z","shell.execute_reply":"2024-10-09T15:37:51.082593Z","shell.execute_reply.started":"2024-10-09T15:37:48.173055Z"},"id":"XxnA_2ZL0qbr","outputId":"2b555cc7-dab0-4a20-f429-94da3171d8a4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import re\n","import nltk\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"aIZOirPU3s3o"},"source":["## Data preparation\n","\n","We load and preprocess the IMDb dataset from the Hugging Face library. The dataset is split into training and testing sets. Tokenization is done using the DistilBERTTokenizer, which converts text into a format suitable for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:37:51.086090Z","iopub.status.busy":"2024-10-09T15:37:51.085638Z","iopub.status.idle":"2024-10-09T15:37:53.190190Z","shell.execute_reply":"2024-10-09T15:37:53.189066Z","shell.execute_reply.started":"2024-10-09T15:37:51.086055Z"},"id":"oNCa6cyb0qbs","trusted":true},"outputs":[],"source":["splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n","df_train = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])\n","df_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:37:53.191774Z","iopub.status.busy":"2024-10-09T15:37:53.191479Z","iopub.status.idle":"2024-10-09T15:37:53.202606Z","shell.execute_reply":"2024-10-09T15:37:53.201588Z","shell.execute_reply.started":"2024-10-09T15:37:53.191743Z"},"id":"LIUKK3je0qbs","trusted":true},"outputs":[],"source":["def clean_text_pipeline(text):\n","    # Lowercase the text\n","    text = text.lower()\n","\n","    # Remove HTML tags\n","    text = re.sub(r'<.*?>', '', text)\n","\n","    # Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+', '', text)\n","\n","    # Remove special characters and punctuation\n","    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n","\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","\n","    # Remove stopwords\n","    stop_words = set(nltk.corpus.stopwords.words('english'))\n","    text = ' '.join([word for word in text.split() if word not in stop_words])\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:37:53.205498Z","iopub.status.busy":"2024-10-09T15:37:53.205090Z","iopub.status.idle":"2024-10-09T15:38:08.361381Z","shell.execute_reply":"2024-10-09T15:38:08.360485Z","shell.execute_reply.started":"2024-10-09T15:37:53.205454Z"},"id":"mKD5MqLO0qbs","trusted":true},"outputs":[],"source":["df_train[\"text_cleaned\"] = df_train[\"text\"].apply(clean_text_pipeline)\n","df_test[\"text_cleaned\"] = df_test[\"text\"].apply(clean_text_pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:38:08.363130Z","iopub.status.busy":"2024-10-09T15:38:08.362723Z","iopub.status.idle":"2024-10-09T15:38:08.371596Z","shell.execute_reply":"2024-10-09T15:38:08.370605Z","shell.execute_reply.started":"2024-10-09T15:38:08.363084Z"},"id":"qlAcJ7kH0qbs","trusted":true},"outputs":[],"source":["class MovieReviewDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        assert len(texts) == len(labels), \"Text and labels must be the same length\"\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        # Tokenize the text with the tokenizer and convert to tensors\n","        encoded_input = self.tokenizer(\n","            text,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        input_ids = encoded_input['input_ids'].squeeze(0)  # Squeeze to remove batch dimension\n","        attention_mask = encoded_input['attention_mask'].squeeze(0)\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask\n","        }, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:38:08.372967Z","iopub.status.busy":"2024-10-09T15:38:08.372692Z","iopub.status.idle":"2024-10-09T15:38:09.708805Z","shell.execute_reply":"2024-10-09T15:38:09.707712Z","shell.execute_reply.started":"2024-10-09T15:38:08.372938Z"},"id":"ucMN0-aJ0qbt","trusted":true},"outputs":[],"source":["from transformers import DistilBertTokenizer, DistilBertModel\n","distilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","for param in distilbert_model.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:38:09.710474Z","iopub.status.busy":"2024-10-09T15:38:09.709996Z","iopub.status.idle":"2024-10-09T15:38:09.720445Z","shell.execute_reply":"2024-10-09T15:38:09.719487Z","shell.execute_reply.started":"2024-10-09T15:38:09.710438Z"},"id":"Hjhf5q1x0qbt","trusted":true},"outputs":[],"source":["max_length = 512  # This is induced by the chosen model, for Distilbert max sequence length is 512, this means that some reviews will be truncated to 512 tokens\n","batch_size = 128\n","\n","# Create dataset\n","dataset_train = MovieReviewDataset(df_train[\"text_cleaned\"].tolist(), df_train[\"label\"].tolist(), tokenizer, max_length)\n","dataset_test = MovieReviewDataset(df_test[\"text_cleaned\"].tolist(), df_test[\"label\"].tolist(), tokenizer, max_length)\n","\n","# Create DataLoader\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"wf-bq6DA4DNi"},"source":["## Model Definition: DistilBERT\n","\n","Here, we define and load the DistilBERT model using the Hugging Face transformers library. We freeze the model's parameters to prevent them from being updated during training and focus on training only the classification head.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:39:02.054470Z","iopub.status.busy":"2024-10-09T15:39:02.053529Z","iopub.status.idle":"2024-10-09T15:39:02.062704Z","shell.execute_reply":"2024-10-09T15:39:02.061773Z","shell.execute_reply.started":"2024-10-09T15:39:02.054427Z"},"id":"fRmlyGeK0qbt","trusted":true},"outputs":[],"source":["# Simple classification model built on top of DistilBERT as feature extractor\n","class DistilBERTClassifier(nn.Module):\n","    def __init__(self, distilbert_model, num_labels):\n","        super(DistilBERTClassifier, self).__init__()\n","        self.distilbert = distilbert_model\n","        self.pre_classifier = nn.Linear(self.distilbert.config.hidden_size, 32)\n","        self.classifier = nn.Linear(32, num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Extract features from DistilBERT\n","        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n","        x = hidden_state[:, 0]  # We take the first token's output (CLS token)\n","        x = self.pre_classifier(x)\n","        x = nn.ReLU()(x)\n","        logits = self.classifier(x)\n","        return logits\n","\n","num_labels = 2\n","model = DistilBERTClassifier(distilbert_model, num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:39:02.481629Z","iopub.status.busy":"2024-10-09T15:39:02.480953Z","iopub.status.idle":"2024-10-09T15:39:02.487120Z","shell.execute_reply":"2024-10-09T15:39:02.486230Z","shell.execute_reply.started":"2024-10-09T15:39:02.481586Z"},"id":"cZpaXeVF0qbt","trusted":true},"outputs":[],"source":["from torch.optim import AdamW\n","optimizer = AdamW(model.parameters(), lr=1e-4)\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:39:02.913701Z","iopub.status.busy":"2024-10-09T15:39:02.913311Z","iopub.status.idle":"2024-10-09T15:39:02.920717Z","shell.execute_reply":"2024-10-09T15:39:02.919706Z","shell.execute_reply.started":"2024-10-09T15:39:02.913664Z"},"id":"b-jkwAd80qbt","outputId":"2665ff4c-1b24-41e6-9338-1952aeaaa231","trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["EPOCHS = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","metadata":{"id":"wUidgfdX4-rP"},"source":["## Model Training\n","### Training on prepared text\n","- Train DistilBERT for 10 epochs.\n","- Track and log performance metrics during training."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T15:39:03.275634Z","iopub.status.busy":"2024-10-09T15:39:03.274876Z","iopub.status.idle":"2024-10-09T17:43:05.144491Z","shell.execute_reply":"2024-10-09T17:43:05.143562Z","shell.execute_reply.started":"2024-10-09T15:39:03.275591Z"},"id":"KiEEsvAd0qbt","outputId":"24fba41d-bf78-4287-9627-2a9ac8c81a54","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:17<00:00,  1.92s/it]\n","100%|██████████| 196/196 [06:05<00:00,  1.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.641500647883026, Accuracy: 0.75356, F1: 0.757316736912593, Precision: 0.7459455264995732, Recall: 0.76904\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:18<00:00,  1.93s/it]\n","100%|██████████| 196/196 [06:05<00:00,  1.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 0.5465788521936962, Accuracy: 0.77864, F1: 0.7796799108209251, Precision: 0.776034236804565, Recall: 0.78336\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:18<00:00,  1.93s/it]\n","100%|██████████| 196/196 [06:05<00:00,  1.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 0.4954750443599662, Accuracy: 0.7878, F1: 0.7804130965685667, Precision: 0.8085599107985247, Recall: 0.75416\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:18<00:00,  1.93s/it]\n","100%|██████████| 196/196 [06:06<00:00,  1.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 0.47062053865924175, Accuracy: 0.79424, F1: 0.7930313028084011, Precision: 0.7977173385138416, Recall: 0.7884\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:19<00:00,  1.93s/it]\n","100%|██████████| 196/196 [06:05<00:00,  1.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 0.45542559423008744, Accuracy: 0.79744, F1: 0.8009903324687573, Precision: 0.7871929553530048, Recall: 0.81528\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:19<00:00,  1.94s/it]\n","100%|██████████| 196/196 [06:05<00:00,  1.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 0.44665515468436845, Accuracy: 0.80316, F1: 0.800535041141421, Precision: 0.8113548599129077, Recall: 0.79\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:17<00:00,  1.93s/it]\n","100%|██████████| 196/196 [06:04<00:00,  1.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 0.441218692277159, Accuracy: 0.80416, F1: 0.7984521653219167, Precision: 0.8224219810040706, Recall: 0.77584\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:18<00:00,  1.93s/it]\n","100%|██████████| 196/196 [06:06<00:00,  1.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 0.43691897377067684, Accuracy: 0.8048, F1: 0.7958329846874739, Precision: 0.8341519031748816, Recall: 0.76088\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:18<00:00,  1.93s/it]\n","100%|██████████| 196/196 [06:06<00:00,  1.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 0.4336636109011514, Accuracy: 0.8084, F1: 0.8108961705487564, Precision: 0.8004676539360873, Recall: 0.8216\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [06:19<00:00,  1.94s/it]\n","100%|██████████| 196/196 [06:04<00:00,  1.86s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 0.4310526201615528, Accuracy: 0.80936, F1: 0.8055328872204995, Precision: 0.8220353097934711, Recall: 0.78968\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model = model.to(device)\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    loss_total = 0\n","    cnt = 0\n","    for batch in tqdm(dataloader_train):\n","        x, y = batch\n","        input_ids = x['input_ids'].to(device)\n","        attention_mask = x['attention_mask'].to(device)\n","        labels = y.to(device)\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        loss_total += loss.item()\n","        cnt += 1\n","\n","    model.eval()\n","    preds = []\n","    for batch in tqdm(dataloader_test):\n","        x, y = batch\n","        input_ids = x['input_ids'].to(device)\n","        attention_mask = x['attention_mask'].to(device)\n","        labels = y.to(device)\n","\n","        # Forward pass\n","        with torch.no_grad():\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        _, predicted = torch.max(outputs, 1)\n","\n","        preds.extend(predicted.cpu().numpy())\n","\n","    acc = accuracy_score(df_test['label'], preds)\n","    f1 = f1_score(df_test['label'], preds)\n","    precision = precision_score(df_test['label'], preds)\n","    recall = recall_score(df_test['label'], preds)\n","\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss_total / cnt}, Accuracy: {acc}, F1: {f1}, Precision: {precision}, Recall: {recall}\")"]},{"cell_type":"markdown","metadata":{"id":"OLXo9HKN5Szr"},"source":["### Training on the base text, with processing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T17:43:05.146530Z","iopub.status.busy":"2024-10-09T17:43:05.146189Z","iopub.status.idle":"2024-10-09T17:43:05.159061Z","shell.execute_reply":"2024-10-09T17:43:05.158147Z","shell.execute_reply.started":"2024-10-09T17:43:05.146496Z"},"id":"sX0nntY10qbu","trusted":true},"outputs":[],"source":["# Create dataset\n","dataset_train = MovieReviewDataset(df_train[\"text\"].tolist(), df_train[\"label\"].tolist(), tokenizer, max_length)\n","dataset_test = MovieReviewDataset(df_test[\"text\"].tolist(), df_test[\"label\"].tolist(), tokenizer, max_length)\n","\n","# Create DataLoader\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T17:43:05.160874Z","iopub.status.busy":"2024-10-09T17:43:05.160445Z","iopub.status.idle":"2024-10-09T17:43:05.173891Z","shell.execute_reply":"2024-10-09T17:43:05.173119Z","shell.execute_reply.started":"2024-10-09T17:43:05.160825Z"},"id":"uJlQ_v5m0qbu","trusted":true},"outputs":[],"source":["num_labels = 2\n","model = DistilBERTClassifier(distilbert_model, num_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T17:43:05.176254Z","iopub.status.busy":"2024-10-09T17:43:05.175910Z","iopub.status.idle":"2024-10-09T17:43:05.185343Z","shell.execute_reply":"2024-10-09T17:43:05.184528Z","shell.execute_reply.started":"2024-10-09T17:43:05.176197Z"},"id":"qYZrcPC10qbu","trusted":true},"outputs":[],"source":["optimizer = AdamW(model.parameters(), lr=1e-4)\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T17:43:05.186557Z","iopub.status.busy":"2024-10-09T17:43:05.186247Z","iopub.status.idle":"2024-10-09T17:43:05.198021Z","shell.execute_reply":"2024-10-09T17:43:05.197242Z","shell.execute_reply.started":"2024-10-09T17:43:05.186521Z"},"id":"6wh8ElOu0qbu","outputId":"170c364e-96ea-47ef-9893-c424128e2d79","trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["EPOCHS = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-09T17:43:05.201372Z","iopub.status.busy":"2024-10-09T17:43:05.200433Z","iopub.status.idle":"2024-10-09T20:13:30.532103Z","shell.execute_reply":"2024-10-09T20:13:30.531128Z","shell.execute_reply.started":"2024-10-09T17:43:05.201338Z"},"id":"45O8x3PW0qbu","outputId":"88f82358-fc68-4431-846b-8b5131919ad1","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:39<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:24<00:00,  2.27s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.6359314924600173, Accuracy: 0.80256, F1: 0.805347424875779, Precision: 0.7941359464924561, Recall: 0.81688\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:38<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:23<00:00,  2.26s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 0.5132269103612218, Accuracy: 0.81948, F1: 0.8170133398207842, Precision: 0.8283318260297624, Recall: 0.806\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:38<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:24<00:00,  2.27s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 0.4375992350432338, Accuracy: 0.83184, F1: 0.8326166587036152, Precision: 0.8287888395688016, Recall: 0.83648\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:39<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:23<00:00,  2.26s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 0.40034028857338183, Accuracy: 0.83936, F1: 0.8383382980436357, Precision: 0.8437044239183277, Recall: 0.83304\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:38<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:24<00:00,  2.27s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 0.3825528227857181, Accuracy: 0.84336, F1: 0.8468637572344752, Precision: 0.8283353733170135, Recall: 0.86624\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:38<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:23<00:00,  2.26s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 0.37013255181361215, Accuracy: 0.8476, F1: 0.8506585136406396, Precision: 0.8339225330464187, Recall: 0.86808\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:37<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:22<00:00,  2.26s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 0.3625328825140486, Accuracy: 0.85064, F1: 0.8533731249509149, Precision: 0.8380379453956501, Recall: 0.86928\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:38<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:24<00:00,  2.27s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 0.3572563136718711, Accuracy: 0.8552, F1: 0.8551304626220586, Precision: 0.8555413196668802, Recall: 0.85472\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:40<00:00,  2.35s/it]\n","100%|██████████| 196/196 [07:24<00:00,  2.27s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 0.35155866583999323, Accuracy: 0.85748, F1: 0.8556496374022606, Precision: 0.8667815808914061, Recall: 0.8448\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [07:38<00:00,  2.34s/it]\n","100%|██████████| 196/196 [07:23<00:00,  2.26s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 0.348458391671278, Accuracy: 0.85648, F1: 0.8584503708379361, Precision: 0.8468244084682441, Recall: 0.8704\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model = model.to(device)\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    loss_total = 0\n","    cnt = 0\n","    for batch in tqdm(dataloader_train):\n","        x, y = batch\n","        input_ids = x['input_ids'].to(device)\n","        attention_mask = x['attention_mask'].to(device)\n","        labels = y.to(device)\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        loss_total += loss.item()\n","        cnt += 1\n","\n","    model.eval()\n","    preds = []\n","    for batch in tqdm(dataloader_test):\n","        x, y = batch\n","        input_ids = x['input_ids'].to(device)\n","        attention_mask = x['attention_mask'].to(device)\n","        labels = y.to(device)\n","\n","        # Forward pass\n","        with torch.no_grad():\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        _, predicted = torch.max(outputs, 1)\n","\n","        preds.extend(predicted.cpu().numpy())\n","\n","    acc = accuracy_score(df_test['label'], preds)\n","    f1 = f1_score(df_test['label'], preds)\n","    precision = precision_score(df_test['label'], preds)\n","    recall = recall_score(df_test['label'], preds)\n","\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss_total / cnt}, Accuracy: {acc}, F1: {f1}, Precision: {precision}, Recall: {recall}\")"]},{"cell_type":"markdown","metadata":{"id":"BPyAtuGg6au_"},"source":["## Summary\n","\n","We use DistilBERT, a smaller version of BERT, to classify movie reviews. Only the classification head is trained while the DistilBERT parameters are frozen.\n","\n","Data-Processing Pipeline:\n","- Tokenization: Text is tokenized using DistilBertTokenizer into input IDs and attention masks.\n","- DataLoader: Data is batched using DataLoaders for efficient training.\n","- Training: Only the classification layer is trained with cross-entropy loss and AdamW optimizer.\n","\n","### Metrics on test setL\n","\n","\n","|            | Accuracy | F1 Score | Precision | Recall |\n","|------------|----------|----------|-----------|--------|\n","| **With preprocessing**  | 0.8084   | 0.8109   | 0.8005    | 0.8216 |\n","| **With preprocessing**   | 0.8575   | 0.8556   | 0.8668    | 0.8448 |\n"]},{"cell_type":"markdown","metadata":{},"source":["Surprisingly, the model performed worse on the processed text compared to the unprocessed version. We believe this is due to how the DistilBERT model was trained—on complete sentences that included numbers and other elements. As a result, processing the text may have altered it too much from the representations the model learned during training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
