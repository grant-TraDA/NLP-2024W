{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvZABs451wcg"
      },
      "source": [
        "# Tutorial 2 - embeddings\n",
        "\n",
        "Authors: Micha≈Ç Gromadzki, Kacper Skonieczka\n",
        "\n",
        "TASK: sentiment analysis classification\n",
        "\n",
        "This project performs sentiment analysis on the IMDb movie reviews dataset using a transformer-based model, DistilBERT. DistilBERT is a smaller, faster variant of BERT and is well-suited for text classification tasks like sentiment analysis. We will preprocess the data, train the model, and evaluate its performance using common classification metrics such as F1 Score, Precision, Recall, and Accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:37:48.173102Z",
          "iopub.status.busy": "2024-10-09T15:37:48.172698Z",
          "iopub.status.idle": "2024-10-09T15:37:51.083702Z",
          "shell.execute_reply": "2024-10-09T15:37:51.082593Z",
          "shell.execute_reply.started": "2024-10-09T15:37:48.173055Z"
        },
        "id": "XxnA_2ZL0qbr",
        "outputId": "2b555cc7-dab0-4a20-f429-94da3171d8a4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIZOirPU3s3o"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "We load and preprocess the IMDb dataset from the Hugging Face library. The dataset is split into training and testing sets. Tokenization is done using the DistilBERTTokenizer, which converts text into a format suitable for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:37:51.086090Z",
          "iopub.status.busy": "2024-10-09T15:37:51.085638Z",
          "iopub.status.idle": "2024-10-09T15:37:53.190190Z",
          "shell.execute_reply": "2024-10-09T15:37:53.189066Z",
          "shell.execute_reply.started": "2024-10-09T15:37:51.086055Z"
        },
        "id": "oNCa6cyb0qbs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])\n",
        "df_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:37:53.191774Z",
          "iopub.status.busy": "2024-10-09T15:37:53.191479Z",
          "iopub.status.idle": "2024-10-09T15:37:53.202606Z",
          "shell.execute_reply": "2024-10-09T15:37:53.201588Z",
          "shell.execute_reply.started": "2024-10-09T15:37:53.191743Z"
        },
        "id": "LIUKK3je0qbs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def clean_text_pipeline(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:37:53.205498Z",
          "iopub.status.busy": "2024-10-09T15:37:53.205090Z",
          "iopub.status.idle": "2024-10-09T15:38:08.361381Z",
          "shell.execute_reply": "2024-10-09T15:38:08.360485Z",
          "shell.execute_reply.started": "2024-10-09T15:37:53.205454Z"
        },
        "id": "mKD5MqLO0qbs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df_train[\"text_cleaned\"] = df_train[\"text\"].apply(clean_text_pipeline)\n",
        "df_test[\"text_cleaned\"] = df_test[\"text\"].apply(clean_text_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:38:08.363130Z",
          "iopub.status.busy": "2024-10-09T15:38:08.362723Z",
          "iopub.status.idle": "2024-10-09T15:38:08.371596Z",
          "shell.execute_reply": "2024-10-09T15:38:08.370605Z",
          "shell.execute_reply.started": "2024-10-09T15:38:08.363084Z"
        },
        "id": "qlAcJ7kH0qbs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MovieReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        assert len(texts) == len(labels), \"Text and labels must be the same length\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Tokenize the text with the tokenizer and convert to tensors\n",
        "        encoded_input = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_input['input_ids'].squeeze(0)  # Squeeze to remove batch dimension\n",
        "        attention_mask = encoded_input['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:38:08.372967Z",
          "iopub.status.busy": "2024-10-09T15:38:08.372692Z",
          "iopub.status.idle": "2024-10-09T15:38:09.708805Z",
          "shell.execute_reply": "2024-10-09T15:38:09.707712Z",
          "shell.execute_reply.started": "2024-10-09T15:38:08.372938Z"
        },
        "id": "ucMN0-aJ0qbt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "distilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "for param in distilbert_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:38:09.710474Z",
          "iopub.status.busy": "2024-10-09T15:38:09.709996Z",
          "iopub.status.idle": "2024-10-09T15:38:09.720445Z",
          "shell.execute_reply": "2024-10-09T15:38:09.719487Z",
          "shell.execute_reply.started": "2024-10-09T15:38:09.710438Z"
        },
        "id": "Hjhf5q1x0qbt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "max_length = 512  # This is induced by the chosen model, for Distilbert max sequence length is 512, this means that some reviews will be truncated to 512 tokens\n",
        "batch_size = 128\n",
        "\n",
        "# Create dataset\n",
        "dataset_train = MovieReviewDataset(df_train[\"text_cleaned\"].tolist(), df_train[\"label\"].tolist(), tokenizer, max_length)\n",
        "dataset_test = MovieReviewDataset(df_test[\"text_cleaned\"].tolist(), df_test[\"label\"].tolist(), tokenizer, max_length)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf-bq6DA4DNi"
      },
      "source": [
        "## Model Definition: DistilBERT\n",
        "\n",
        "Here, we define and load the DistilBERT model using the Hugging Face transformers library. We freeze the model's parameters to prevent them from being updated during training and focus on training only the classification head.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:39:02.054470Z",
          "iopub.status.busy": "2024-10-09T15:39:02.053529Z",
          "iopub.status.idle": "2024-10-09T15:39:02.062704Z",
          "shell.execute_reply": "2024-10-09T15:39:02.061773Z",
          "shell.execute_reply.started": "2024-10-09T15:39:02.054427Z"
        },
        "id": "fRmlyGeK0qbt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Simple classification model built on top of DistilBERT as feature extractor\n",
        "class DistilBERTClassifier(nn.Module):\n",
        "    def __init__(self, distilbert_model, num_labels):\n",
        "        super(DistilBERTClassifier, self).__init__()\n",
        "        self.distilbert = distilbert_model\n",
        "        self.pre_classifier = nn.Linear(self.distilbert.config.hidden_size, 32)\n",
        "        self.classifier = nn.Linear(32, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Extract features from DistilBERT\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
        "        x = hidden_state[:, 0]  # We take the first token's output (CLS token)\n",
        "        x = self.pre_classifier(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "num_labels = 2\n",
        "model = DistilBERTClassifier(distilbert_model, num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:39:02.481629Z",
          "iopub.status.busy": "2024-10-09T15:39:02.480953Z",
          "iopub.status.idle": "2024-10-09T15:39:02.487120Z",
          "shell.execute_reply": "2024-10-09T15:39:02.486230Z",
          "shell.execute_reply.started": "2024-10-09T15:39:02.481586Z"
        },
        "id": "cZpaXeVF0qbt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:39:02.913701Z",
          "iopub.status.busy": "2024-10-09T15:39:02.913311Z",
          "iopub.status.idle": "2024-10-09T15:39:02.920717Z",
          "shell.execute_reply": "2024-10-09T15:39:02.919706Z",
          "shell.execute_reply.started": "2024-10-09T15:39:02.913664Z"
        },
        "id": "b-jkwAd80qbt",
        "outputId": "2665ff4c-1b24-41e6-9338-1952aeaaa231",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUidgfdX4-rP"
      },
      "source": [
        "## Model Training\n",
        "### Training on prepared text\n",
        "- Train DistilBERT for 10 epochs.\n",
        "- Track and log performance metrics during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T15:39:03.275634Z",
          "iopub.status.busy": "2024-10-09T15:39:03.274876Z",
          "iopub.status.idle": "2024-10-09T17:43:05.144491Z",
          "shell.execute_reply": "2024-10-09T17:43:05.143562Z",
          "shell.execute_reply.started": "2024-10-09T15:39:03.275591Z"
        },
        "id": "KiEEsvAd0qbt",
        "outputId": "24fba41d-bf78-4287-9627-2a9ac8c81a54",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:17<00:00,  1.92s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:05<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.641500647883026, Accuracy: 0.75356, F1: 0.757316736912593, Precision: 0.7459455264995732, Recall: 0.76904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:18<00:00,  1.93s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:05<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Loss: 0.5465788521936962, Accuracy: 0.77864, F1: 0.7796799108209251, Precision: 0.776034236804565, Recall: 0.78336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:18<00:00,  1.93s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:05<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10, Loss: 0.4954750443599662, Accuracy: 0.7878, F1: 0.7804130965685667, Precision: 0.8085599107985247, Recall: 0.75416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:18<00:00,  1.93s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:06<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Loss: 0.47062053865924175, Accuracy: 0.79424, F1: 0.7930313028084011, Precision: 0.7977173385138416, Recall: 0.7884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:19<00:00,  1.93s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:05<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10, Loss: 0.45542559423008744, Accuracy: 0.79744, F1: 0.8009903324687573, Precision: 0.7871929553530048, Recall: 0.81528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:19<00:00,  1.94s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:05<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10, Loss: 0.44665515468436845, Accuracy: 0.80316, F1: 0.800535041141421, Precision: 0.8113548599129077, Recall: 0.79\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:17<00:00,  1.93s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:04<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10, Loss: 0.441218692277159, Accuracy: 0.80416, F1: 0.7984521653219167, Precision: 0.8224219810040706, Recall: 0.77584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:18<00:00,  1.93s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:06<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10, Loss: 0.43691897377067684, Accuracy: 0.8048, F1: 0.7958329846874739, Precision: 0.8341519031748816, Recall: 0.76088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:18<00:00,  1.93s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:06<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10, Loss: 0.4336636109011514, Accuracy: 0.8084, F1: 0.8108961705487564, Precision: 0.8004676539360873, Recall: 0.8216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:19<00:00,  1.94s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [06:04<00:00,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, Loss: 0.4310526201615528, Accuracy: 0.80936, F1: 0.8055328872204995, Precision: 0.8220353097934711, Recall: 0.78968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    loss_total = 0\n",
        "    cnt = 0\n",
        "    for batch in tqdm(dataloader_train):\n",
        "        x, y = batch\n",
        "        input_ids = x['input_ids'].to(device)\n",
        "        attention_mask = x['attention_mask'].to(device)\n",
        "        labels = y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_total += loss.item()\n",
        "        cnt += 1\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for batch in tqdm(dataloader_test):\n",
        "        x, y = batch\n",
        "        input_ids = x['input_ids'].to(device)\n",
        "        attention_mask = x['attention_mask'].to(device)\n",
        "        labels = y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(df_test['label'], preds)\n",
        "    f1 = f1_score(df_test['label'], preds)\n",
        "    precision = precision_score(df_test['label'], preds)\n",
        "    recall = recall_score(df_test['label'], preds)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss_total / cnt}, Accuracy: {acc}, F1: {f1}, Precision: {precision}, Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLXo9HKN5Szr"
      },
      "source": [
        "### Training on the base text, with processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T17:43:05.146530Z",
          "iopub.status.busy": "2024-10-09T17:43:05.146189Z",
          "iopub.status.idle": "2024-10-09T17:43:05.159061Z",
          "shell.execute_reply": "2024-10-09T17:43:05.158147Z",
          "shell.execute_reply.started": "2024-10-09T17:43:05.146496Z"
        },
        "id": "sX0nntY10qbu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "dataset_train = MovieReviewDataset(df_train[\"text\"].tolist(), df_train[\"label\"].tolist(), tokenizer, max_length)\n",
        "dataset_test = MovieReviewDataset(df_test[\"text\"].tolist(), df_test[\"label\"].tolist(), tokenizer, max_length)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T17:43:05.160874Z",
          "iopub.status.busy": "2024-10-09T17:43:05.160445Z",
          "iopub.status.idle": "2024-10-09T17:43:05.173891Z",
          "shell.execute_reply": "2024-10-09T17:43:05.173119Z",
          "shell.execute_reply.started": "2024-10-09T17:43:05.160825Z"
        },
        "id": "uJlQ_v5m0qbu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_labels = 2\n",
        "model = DistilBERTClassifier(distilbert_model, num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T17:43:05.176254Z",
          "iopub.status.busy": "2024-10-09T17:43:05.175910Z",
          "iopub.status.idle": "2024-10-09T17:43:05.185343Z",
          "shell.execute_reply": "2024-10-09T17:43:05.184528Z",
          "shell.execute_reply.started": "2024-10-09T17:43:05.176197Z"
        },
        "id": "qYZrcPC10qbu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T17:43:05.186557Z",
          "iopub.status.busy": "2024-10-09T17:43:05.186247Z",
          "iopub.status.idle": "2024-10-09T17:43:05.198021Z",
          "shell.execute_reply": "2024-10-09T17:43:05.197242Z",
          "shell.execute_reply.started": "2024-10-09T17:43:05.186521Z"
        },
        "id": "6wh8ElOu0qbu",
        "outputId": "170c364e-96ea-47ef-9893-c424128e2d79",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-09T17:43:05.201372Z",
          "iopub.status.busy": "2024-10-09T17:43:05.200433Z",
          "iopub.status.idle": "2024-10-09T20:13:30.532103Z",
          "shell.execute_reply": "2024-10-09T20:13:30.531128Z",
          "shell.execute_reply.started": "2024-10-09T17:43:05.201338Z"
        },
        "id": "45O8x3PW0qbu",
        "outputId": "88f82358-fc68-4431-846b-8b5131919ad1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:39<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:24<00:00,  2.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.6359314924600173, Accuracy: 0.80256, F1: 0.805347424875779, Precision: 0.7941359464924561, Recall: 0.81688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:38<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:23<00:00,  2.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10, Loss: 0.5132269103612218, Accuracy: 0.81948, F1: 0.8170133398207842, Precision: 0.8283318260297624, Recall: 0.806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:38<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:24<00:00,  2.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10, Loss: 0.4375992350432338, Accuracy: 0.83184, F1: 0.8326166587036152, Precision: 0.8287888395688016, Recall: 0.83648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:39<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:23<00:00,  2.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10, Loss: 0.40034028857338183, Accuracy: 0.83936, F1: 0.8383382980436357, Precision: 0.8437044239183277, Recall: 0.83304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:38<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:24<00:00,  2.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10, Loss: 0.3825528227857181, Accuracy: 0.84336, F1: 0.8468637572344752, Precision: 0.8283353733170135, Recall: 0.86624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:38<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:23<00:00,  2.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10, Loss: 0.37013255181361215, Accuracy: 0.8476, F1: 0.8506585136406396, Precision: 0.8339225330464187, Recall: 0.86808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:37<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:22<00:00,  2.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10, Loss: 0.3625328825140486, Accuracy: 0.85064, F1: 0.8533731249509149, Precision: 0.8380379453956501, Recall: 0.86928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:38<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:24<00:00,  2.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10, Loss: 0.3572563136718711, Accuracy: 0.8552, F1: 0.8551304626220586, Precision: 0.8555413196668802, Recall: 0.85472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:40<00:00,  2.35s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:24<00:00,  2.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10, Loss: 0.35155866583999323, Accuracy: 0.85748, F1: 0.8556496374022606, Precision: 0.8667815808914061, Recall: 0.8448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:38<00:00,  2.34s/it]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 196/196 [07:23<00:00,  2.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, Loss: 0.348458391671278, Accuracy: 0.85648, F1: 0.8584503708379361, Precision: 0.8468244084682441, Recall: 0.8704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    loss_total = 0\n",
        "    cnt = 0\n",
        "    for batch in tqdm(dataloader_train):\n",
        "        x, y = batch\n",
        "        input_ids = x['input_ids'].to(device)\n",
        "        attention_mask = x['attention_mask'].to(device)\n",
        "        labels = y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_total += loss.item()\n",
        "        cnt += 1\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    for batch in tqdm(dataloader_test):\n",
        "        x, y = batch\n",
        "        input_ids = x['input_ids'].to(device)\n",
        "        attention_mask = x['attention_mask'].to(device)\n",
        "        labels = y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(df_test['label'], preds)\n",
        "    f1 = f1_score(df_test['label'], preds)\n",
        "    precision = precision_score(df_test['label'], preds)\n",
        "    recall = recall_score(df_test['label'], preds)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss_total / cnt}, Accuracy: {acc}, F1: {f1}, Precision: {precision}, Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPyAtuGg6au_"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We use DistilBERT, a smaller version of BERT, to classify movie reviews. Only the classification head is trained while the DistilBERT parameters are frozen.\n",
        "\n",
        "Data-Processing Pipeline:\n",
        "- Tokenization: Text is tokenized using DistilBertTokenizer into input IDs and attention masks.\n",
        "- DataLoader: Data is batched using DataLoaders for efficient training.\n",
        "- Training: Only the classification layer is trained with cross-entropy loss and AdamW optimizer.\n",
        "\n",
        "### Metrics on test setL\n",
        "\n",
        "\n",
        "|            | Accuracy | F1 Score | Precision | Recall |\n",
        "|------------|----------|----------|-----------|--------|\n",
        "| **With preprocessing**  | 0.8084   | 0.8109   | 0.8005    | 0.8216 |\n",
        "| **Without preprocessing**   | 0.8575   | 0.8556   | 0.8668    | 0.8448 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yU_45ZcxDWJ"
      },
      "source": [
        "Surprisingly, the model performed worse on the processed text compared to the unprocessed version. We believe this is due to how the DistilBERT model was trained‚Äîon complete sentences that included numbers and other elements. As a result, processing the text may have altered it too much from the representations the model learned during training."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}