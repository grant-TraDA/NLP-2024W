{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                               \n",
    "from nltk.corpus import twitter_samples   \n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt      \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split \n",
    "from nltk.classify import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maaja\\anaconda3\\envs\\supvis\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])\n",
    "df_test =  pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>A hit at the time but now better categorised a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I love this movie like no other. Another time ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>This film and it's sequel Barry Mckenzie holds...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>'The Adventures Of Barry McKenzie' started lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The story centers around Barry McKenzie who mu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2      If only to avoid making this type of film in t...      0\n",
       "3      This film was probably inspired by Godard's Ma...      0\n",
       "4      Oh, brother...after hearing about this ridicul...      0\n",
       "...                                                  ...    ...\n",
       "24995  A hit at the time but now better categorised a...      1\n",
       "24996  I love this movie like no other. Another time ...      1\n",
       "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
       "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
       "24999  The story centers around Barry McKenzie who mu...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "df_train['tokens'] = df_train['text'].apply(preprocess_text)\n",
    "X_train, y_train = df_train['tokens'], df_train['label']\n",
    "\n",
    "df_test['tokens'] = df_test['text'].apply(preprocess_text)\n",
    "X_test, y_test = df_test['tokens'], df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [i, rented, i, am, curious-yellow, from, my, v...\n",
       "1        [``, i, am, curious, :, yellow, '', is, a, ris...\n",
       "2        [if, only, to, avoid, making, this, type, of, ...\n",
       "3        [this, film, was, probably, inspired, by, goda...\n",
       "4        [oh, ,, brother, ..., after, hearing, about, t...\n",
       "                               ...                        \n",
       "24995    [a, hit, at, the, time, but, now, better, cate...\n",
       "24996    [i, love, this, movie, like, no, other, ., ano...\n",
       "24997    [this, film, and, it, 's, sequel, barry, mcken...\n",
       "24998    ['the, adventures, of, barry, mckenzie, ', sta...\n",
       "24999    [the, story, centers, around, barry, mckenzie,...\n",
       "Name: tokens, Length: 25000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of different methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "X_train_clean = [remove_stopwords(tokens) for tokens in X_train]\n",
    "X_test_clean = [remove_stopwords(tokens) for tokens in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "def vectorize_sentence_w2v(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return sum(vectors) / len(vectors) if vectors else [0] * 100\n",
    "\n",
    "X_train_w2v = [vectorize_sentence_w2v(tokens, w2v_model) for tokens in X_train_clean]\n",
    "X_test_w2v = [vectorize_sentence_w2v(tokens, w2v_model) for tokens in X_test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "fasttext_model.save(\"fasttext.model\")\n",
    "fasttext_model = FastText.load(\"fasttext.model\")\n",
    "\n",
    "X_train_fasttext = [vectorize_sentence_w2v(tokens, fasttext_model) for tokens in X_train_clean]\n",
    "X_test_fasttext = [vectorize_sentence_w2v(tokens, fasttext_model) for tokens in X_test_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results comparison for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, model, method_name, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"Method Name\": method_name,\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "results.append(evaluate_model(X_train_w2v, y_train, X_test_w2v, y_test, LogisticRegression(max_iter=2000), \"Word2Vec\",\"LogisticRegression\"))\n",
    "results.append(evaluate_model(X_train_fasttext, y_train, X_test_fasttext, y_test, LogisticRegression(max_iter=2000), \"FastText\",\"LogisticRegression\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "results.append(evaluate_model(X_train_w2v, y_train, X_test_w2v, y_test, RandomForestClassifier(n_estimators=100), \"Word2Vec\", \"RandomForest\"))\n",
    "results.append(evaluate_model(X_train_fasttext, y_train, X_test_fasttext, y_test, RandomForestClassifier(n_estimators=100), \"FastText\", \"RandomForest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "results.append(evaluate_model(X_train_w2v, y_train, X_test_w2v, y_test, SVC(kernel='linear'), \"Word2Vec\", \"SVM\"))\n",
    "results.append(evaluate_model(X_train_fasttext, y_train, X_test_fasttext, y_test, SVC(kernel='linear'), \"FastText\", \"SVM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "results.append(evaluate_model(X_train_w2v, y_train, X_test_w2v, y_test, KNeighborsClassifier(n_neighbors=5), \"Word2Vec\", \"KNN\"))\n",
    "results.append(evaluate_model(X_train_fasttext, y_train, X_test_fasttext, y_test, KNeighborsClassifier(n_neighbors=5), \"FastText\", \"KNN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "results.append(evaluate_model(X_train_w2v, y_train, X_test_w2v, y_test, GaussianNB(), \"Word2Vec\", \"NaiveBayes\"))\n",
    "results.append(evaluate_model(X_train_fasttext, y_train, X_test_fasttext, y_test, GaussianNB(), \"FastText\", \"NaiveBayes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "results.append(evaluate_model(X_train_w2v, y_train, X_test_w2v, y_test, XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), \"Word2Vec\", \"XGBoost\"))\n",
    "results.append(evaluate_model(X_train_fasttext, y_train, X_test_fasttext, y_test, XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), \"FastText\", \"XGBoost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Method Name               Model  Accuracy  Precision   Recall  F1 Score\n",
      "0     Word2Vec  LogisticRegression   0.81320   0.811159  0.81648  0.813811\n",
      "1     FastText  LogisticRegression   0.80008   0.799362  0.80128  0.800320\n",
      "2     Word2Vec        RandomForest   0.75792   0.764653  0.74520  0.754801\n",
      "3     FastText        RandomForest   0.71212   0.717747  0.69920  0.708352\n",
      "4     Word2Vec                 SVM   0.81176   0.807771  0.81824  0.812972\n",
      "5     FastText                 SVM   0.80052   0.796277  0.80768  0.801938\n",
      "6     Word2Vec                 KNN   0.69340   0.721606  0.62976  0.672562\n",
      "7     FastText                 KNN   0.62640   0.641882  0.57184  0.604840\n",
      "8     Word2Vec          NaiveBayes   0.62756   0.620695  0.65600  0.637859\n",
      "9     FastText          NaiveBayes   0.54268   0.532596  0.69736  0.603942\n",
      "10    Word2Vec             XGBoost   0.78232   0.785287  0.77712  0.781182\n",
      "11    FastText             XGBoost   0.74020   0.742548  0.73536  0.738936\n"
     ]
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(results)\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
