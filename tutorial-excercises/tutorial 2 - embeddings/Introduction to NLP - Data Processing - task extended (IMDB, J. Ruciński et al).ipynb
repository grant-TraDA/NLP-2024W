{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import gensim.downloader as api\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])\n",
    "df_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def load_glove_vectors(vocab, glove_model):\n",
    "    return {word: glove_model[word] for word in vocab if word in glove_model}\n",
    "\n",
    "def vectorize(text, model):\n",
    "    vec = np.mean([model[word] for word in text if word in model], axis=0)\n",
    "    return vec if vec is not np.nan else np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply preprocessing\n",
    "df['text'] = df['text'].apply(preprocess)\n",
    "df_test['text'] = df_test['text'].apply(preprocess)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train = X_train.apply(tokenize)\n",
    "X_val = X_val.apply(tokenize)\n",
    "X_test = df_test['text'].apply(tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "X_train_w2v = np.array([vectorize(sentence, w2v_model.wv) for sentence in X_train])\n",
    "X_val_w2v = np.array([vectorize(sentence, w2v_model.wv) for sentence in X_val])\n",
    "X_test_w2v = np.array([vectorize(sentence, w2v_model.wv) for sentence in X_test])\n",
    "\n",
    "clf_w2v = LogisticRegression(solver='saga')\n",
    "clf_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_w2v = clf_w2v.predict(X_val_w2v)\n",
    "\n",
    "precision_w2v = precision_score(y_val, y_pred_w2v)\n",
    "recall_w2v = recall_score(y_val, y_pred_w2v)\n",
    "f1_w2v = f1_score(y_val, y_pred_w2v)\n",
    "accuracy_w2v = accuracy_score(y_val, y_pred_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ft_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "X_train_ft = np.array([vectorize(sentence, ft_model.wv) for sentence in X_train])\n",
    "X_val_ft = np.array([vectorize(sentence, ft_model.wv) for sentence in X_val])\n",
    "X_test_ft = np.array([vectorize(sentence, ft_model.wv) for sentence in X_test])\n",
    "\n",
    "clf_ft = LogisticRegression(solver='saga')\n",
    "clf_ft.fit(X_train_ft, y_train)\n",
    "y_pred_ft = clf_ft.predict(X_val_ft)\n",
    "\n",
    "precision_ft = precision_score(y_val, y_pred_ft)\n",
    "recall_ft = recall_score(y_val, y_pred_ft)\n",
    "f1_ft = f1_score(y_val, y_pred_ft)\n",
    "accuracy_ft = accuracy_score(y_val, y_pred_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(word for sentence in X_train for word in sentence)\n",
    "glove_vectors = load_glove_vectors(vocab, api.load(\"glove-wiki-gigaword-100\"))\n",
    "\n",
    "\n",
    "X_train_glove = np.array([vectorize(sentence, glove_vectors) for sentence in X_train])\n",
    "X_val_glove = np.array([vectorize(sentence, glove_vectors) for sentence in X_val])\n",
    "X_test_glove = np.array([vectorize(sentence, glove_vectors) for sentence in X_test])\n",
    "\n",
    "clf_glove = LogisticRegression(solver='saga')\n",
    "clf_glove.fit(X_train_glove, y_train)\n",
    "y_pred_glove = clf_glove.predict(X_val_glove)\n",
    "\n",
    "precision_glove = precision_score(y_val, y_pred_glove)\n",
    "recall_glove = recall_score(y_val, y_pred_glove)\n",
    "f1_glove = f1_score(y_val, y_pred_glove)\n",
    "accuracy_glove = accuracy_score(y_val, y_pred_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Precision    Recall  F1-Score  Accuracy\n",
      "0  Word2Vec   0.810758  0.830986  0.820747    0.8196\n",
      "1  FastText   0.808864  0.822535  0.815642    0.8152\n",
      "2     GloVe   0.790985  0.797988  0.794471    0.7948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Word2Vec', 'FastText', 'GloVe'],\n",
    "    'Precision': [precision_w2v, precision_ft, precision_glove],\n",
    "    'Recall': [recall_w2v, recall_ft, recall_glove],\n",
    "    'F1-Score': [f1_w2v, f1_ft, f1_glove],\n",
    "    'Accuracy': [accuracy_w2v, accuracy_ft, accuracy_glove]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "\n",
    "del w2v_model, ft_model, clf_w2v, clf_ft, clf_glove, X_train_w2v, X_val_w2v, X_test_w2v\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same but with parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Word2Vec params: {'vector_size': 200, 'window': 10}, Score: 0.8436\n",
      "Best FastText params: {'vector_size': 200, 'window': 10}, Score: 0.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Precision    Recall  F1-Score  Accuracy\n",
      "0  Word2Vec   0.833929  0.844668  0.839264    0.8392\n",
      "1  FastText   0.820492  0.831388  0.825904    0.8258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jedre\\miniforge-pypy3\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "w2v_params = {'vector_size': [100, 200], 'window': [5, 10]}\n",
    "ft_params = {'vector_size': [100, 200], 'window': [5, 10]}\n",
    "\n",
    "def train_evaluate_w2v(params):\n",
    "    model = Word2Vec(sentences=X_train, vector_size=params['vector_size'], window=params['window'], min_count=5, workers=4)\n",
    "    X_train_vec = np.array([vectorize(sentence, model.wv) for sentence in X_train])\n",
    "    X_val_vec = np.array([vectorize(sentence, model.wv) for sentence in X_val])\n",
    "    clf = LogisticRegression(solver='saga')\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_val_vec)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "def train_evaluate_ft(params):\n",
    "    model = FastText(sentences=X_train, vector_size=params['vector_size'], window=params['window'], min_count=5, workers=4)\n",
    "    X_train_vec = np.array([vectorize(sentence, model.wv) for sentence in X_train])\n",
    "    X_val_vec = np.array([vectorize(sentence, model.wv) for sentence in X_val])\n",
    "    clf = LogisticRegression(solver='saga')\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_val_vec)\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "best_w2v_params = None\n",
    "best_w2v_score = 0\n",
    "for vector_size in w2v_params['vector_size']:\n",
    "    for window in w2v_params['window']:\n",
    "        params = {'vector_size': vector_size, 'window': window}\n",
    "        score = train_evaluate_w2v(params)\n",
    "        if score > best_w2v_score:\n",
    "            best_w2v_score = score\n",
    "            best_w2v_params = params\n",
    "\n",
    "best_ft_params = None\n",
    "best_ft_score = 0\n",
    "for vector_size in ft_params['vector_size']:\n",
    "    for window in ft_params['window']:\n",
    "        params = {'vector_size': vector_size, 'window': window}\n",
    "        score = train_evaluate_ft(params)\n",
    "        if score > best_ft_score:\n",
    "            best_ft_score = score\n",
    "            best_ft_params = params\n",
    "\n",
    "print(f\"Best Word2Vec params: {best_w2v_params}, Score: {best_w2v_score}\")\n",
    "print(f\"Best FastText params: {best_ft_params}, Score: {best_ft_score}\")\n",
    "\n",
    "w2v_model = Word2Vec(sentences=X_train, vector_size=best_w2v_params['vector_size'], window=best_w2v_params['window'], min_count=5, workers=4)\n",
    "ft_model = FastText(sentences=X_train, vector_size=best_ft_params['vector_size'], window=best_ft_params['window'], min_count=5, workers=4)\n",
    "\n",
    "X_train_w2v = np.array([vectorize(sentence, w2v_model.wv) for sentence in X_train])\n",
    "X_val_w2v = np.array([vectorize(sentence, w2v_model.wv) for sentence in X_val])\n",
    "X_test_w2v = np.array([vectorize(sentence, w2v_model.wv) for sentence in X_test])\n",
    "\n",
    "X_train_ft = np.array([vectorize(sentence, ft_model.wv) for sentence in X_train])\n",
    "X_val_ft = np.array([vectorize(sentence, ft_model.wv) for sentence in X_val])\n",
    "X_test_ft = np.array([vectorize(sentence, ft_model.wv) for sentence in X_test])\n",
    "\n",
    "clf_w2v = LogisticRegression(solver='saga')\n",
    "clf_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_w2v = clf_w2v.predict(X_val_w2v)\n",
    "\n",
    "clf_ft = LogisticRegression(solver='saga')\n",
    "clf_ft.fit(X_train_ft, y_train)\n",
    "y_pred_ft = clf_ft.predict(X_val_ft)\n",
    "\n",
    "precision_w2v = precision_score(y_val, y_pred_w2v)\n",
    "recall_w2v = recall_score(y_val, y_pred_w2v)\n",
    "f1_w2v = f1_score(y_val, y_pred_w2v)\n",
    "accuracy_w2v = accuracy_score(y_val, y_pred_w2v)\n",
    "\n",
    "precision_ft = precision_score(y_val, y_pred_ft)\n",
    "recall_ft = recall_score(y_val, y_pred_ft)\n",
    "f1_ft = f1_score(y_val, y_pred_ft)\n",
    "accuracy_ft = accuracy_score(y_val, y_pred_ft)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Word2Vec', 'FastText'],\n",
    "    'Precision': [precision_w2v, precision_ft],\n",
    "    'Recall': [recall_w2v, recall_ft],\n",
    "    'F1-Score': [f1_w2v, f1_ft],\n",
    "    'Accuracy': [accuracy_w2v, accuracy_ft]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model  Precision    Recall  F1-Score  Accuracy\n",
      "0  Word2Vec   0.833929  0.844668  0.839264    0.8392\n",
      "1  FastText   0.820492  0.831388  0.825904    0.8258\n"
     ]
    }
   ],
   "source": [
    "# Display the results DataFrame\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
