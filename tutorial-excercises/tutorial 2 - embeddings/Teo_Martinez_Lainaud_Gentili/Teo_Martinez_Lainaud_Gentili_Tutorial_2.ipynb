{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Embeddings\n",
    "\n",
    "Done by:\n",
    "- Nigel Teo\n",
    "- Marc Martinez\n",
    "- Menon Lainaud\n",
    "- Alessandro Gentili\n",
    "\n",
    "## TASK: sentiment analysis classification\n",
    "\n",
    "Dataset: IMBD dataset with the defined split  into train/test,\n",
    "e.g. from here:  https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "\n",
    "Methods: try different methods with embedding-based models, \n",
    "i.e. word2vec, fasttext, glove, transformers etc. \n",
    "\n",
    "\n",
    "Outcome: table with metrics for all tested models \n",
    "and data-processing pipelines (F1 score, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anadu\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "\n",
    "\n",
    "# Import word embedding models (word2vec, fasttext, glove, transformers)\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "# Import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Import sklearn libraries and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anadu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset from Dataset: IMBD dataset with the defined split  into train/test,\n",
    "# e.g. from here:  https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "\n",
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rented curious yellow video store controversy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>curious yellow risible pretentious steaming pi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avoid making type film future film interesting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film probably inspired godard masculin féminin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh brother hearing ridiculous film umpteen yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  rented curious yellow video store controversy ...      0\n",
       "1  curious yellow risible pretentious steaming pi...      0\n",
       "2  avoid making type film future film interesting...      0\n",
       "3  film probably inspired godard masculin féminin...      0\n",
       "4  oh brother hearing ridiculous film umpteen yea...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    # Converting to Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stop words using NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Apply the preprocess_text function to the 'text' column\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data using DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "X_train_encoded = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "X_test_encoded = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Convert the encoded data into PyTorch tensors\n",
    "X_train_input_ids = torch.tensor(X_train_encoded['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X_train.apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.train(X_train.apply(lambda x: x.split()), total_examples=word2vec_model.corpus_count, epochs=10)\n",
    "\n",
    "# Encode the text data using the Word2Vec model\n",
    "X_train_word2vec = X_train.apply(lambda x: np.mean([word2vec_model.wv[word] for word in x.split() if word in word2vec_model.wv], axis=0))\n",
    "X_test_word2vec = X_test.apply(lambda x: np.mean([word2vec_model.wv[word] for word in x.split() if word in word2vec_model.wv], axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: SVM\n",
      "Accuracy: 0.86680\n",
      "F1 Score: 0.86780\n",
      "Precision: 0.85625\n",
      "Recall: 0.87968\n",
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.86320\n",
      "F1 Score: 0.86396\n",
      "Precision: 0.85411\n",
      "Recall: 0.87404\n",
      "Classifier: Random Forest\n",
      "Accuracy: 0.83640\n",
      "F1 Score: 0.83866\n",
      "Precision: 0.82244\n",
      "Recall: 0.85553\n",
      "Classifier: Gradient Boosting\n",
      "Accuracy: 0.84160\n",
      "F1 Score: 0.84185\n",
      "Precision: 0.83551\n",
      "Recall: 0.84829\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word2vec + SVM</td>\n",
       "      <td>0.8668</td>\n",
       "      <td>0.86780</td>\n",
       "      <td>0.85625</td>\n",
       "      <td>0.87968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word2vec + Logistic Regression</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.86396</td>\n",
       "      <td>0.85411</td>\n",
       "      <td>0.87404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word2vec + Random Forest</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.83866</td>\n",
       "      <td>0.82244</td>\n",
       "      <td>0.85553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word2vec + Gradient Boosting</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>0.84185</td>\n",
       "      <td>0.83551</td>\n",
       "      <td>0.84829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Classifier  Accuracy  F1 Score  Precision   Recall\n",
       "0                  word2vec + SVM    0.8668   0.86780    0.85625  0.87968\n",
       "1  word2vec + Logistic Regression    0.8632   0.86396    0.85411  0.87404\n",
       "2        word2vec + Random Forest    0.8364   0.83866    0.82244  0.85553\n",
       "3    word2vec + Gradient Boosting    0.8416   0.84185    0.83551  0.84829"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Word2Vec model using classifiers with statistics\n",
    "\n",
    "classifiers = {}\n",
    "classifiers['SVM'] = SVC()\n",
    "classifiers['Logistic Regression'] = LogisticRegression()\n",
    "classifiers['Random Forest'] = RandomForestClassifier()\n",
    "classifiers['Gradient Boosting'] = GradientBoostingClassifier()\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Classifier', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_word2vec.tolist(), y_train)\n",
    "    y_pred = classifier.predict(X_test_word2vec.tolist())\n",
    "    print(f'Classifier: {name}')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.5f}')\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred):.5f}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred):.5f}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred):.5f}')\n",
    "    \n",
    "    # Create a DataFrame with values rounded to 5 decimal places\n",
    "    df = pd.DataFrame([[\n",
    "        \"word2vec + \" + name, \n",
    "        round(accuracy_score(y_test, y_pred), 5), \n",
    "        round(f1_score(y_test, y_pred), 5), \n",
    "        round(precision_score(y_test, y_pred), 5), \n",
    "        round(recall_score(y_test, y_pred), 5)\n",
    "    ]], columns=['Classifier', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "    \n",
    "    results_df = pd.concat([results_df, df], ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the FastText model\n",
    "fasttext_model = FastText(sentences=X_train.apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
    "fasttext_model.train(X_train.apply(lambda x: x.split()), total_examples=fasttext_model.corpus_count, epochs=10)\n",
    "\n",
    "# Encode the text data using the FastText model\n",
    "X_train_fasttext = X_train.apply(lambda x: np.mean([fasttext_model.wv[word] for word in x.split() if word in fasttext_model.wv], axis=0))\n",
    "X_test_fasttext = X_test.apply(lambda x: np.mean([fasttext_model.wv[word] for word in x.split() if word in fasttext_model.wv], axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: SVM\n",
      "Accuracy: 0.85760\n",
      "F1 Score: 0.85788\n",
      "Precision: 0.85109\n",
      "Recall: 0.86479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.85340\n",
      "F1 Score: 0.85314\n",
      "Precision: 0.84956\n",
      "Recall: 0.85674\n",
      "Classifier: Random Forest\n",
      "Accuracy: 0.81320\n",
      "F1 Score: 0.81585\n",
      "Precision: 0.79977\n",
      "Recall: 0.83260\n",
      "Classifier: Gradient Boosting\n",
      "Accuracy: 0.82720\n",
      "F1 Score: 0.82823\n",
      "Precision: 0.81847\n",
      "Recall: 0.83823\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word2vec + SVM</td>\n",
       "      <td>0.8668</td>\n",
       "      <td>0.86780</td>\n",
       "      <td>0.85625</td>\n",
       "      <td>0.87968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word2vec + Logistic Regression</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.86396</td>\n",
       "      <td>0.85411</td>\n",
       "      <td>0.87404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word2vec + Random Forest</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.83866</td>\n",
       "      <td>0.82244</td>\n",
       "      <td>0.85553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word2vec + Gradient Boosting</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>0.84185</td>\n",
       "      <td>0.83551</td>\n",
       "      <td>0.84829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fasttext + SVM</td>\n",
       "      <td>0.8576</td>\n",
       "      <td>0.85788</td>\n",
       "      <td>0.85109</td>\n",
       "      <td>0.86479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fasttext + Logistic Regression</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>0.85314</td>\n",
       "      <td>0.84956</td>\n",
       "      <td>0.85674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fasttext + Random Forest</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.81585</td>\n",
       "      <td>0.79977</td>\n",
       "      <td>0.83260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fasttext + Gradient Boosting</td>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.82823</td>\n",
       "      <td>0.81847</td>\n",
       "      <td>0.83823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Classifier  Accuracy  F1 Score  Precision   Recall\n",
       "0                  word2vec + SVM    0.8668   0.86780    0.85625  0.87968\n",
       "1  word2vec + Logistic Regression    0.8632   0.86396    0.85411  0.87404\n",
       "2        word2vec + Random Forest    0.8364   0.83866    0.82244  0.85553\n",
       "3    word2vec + Gradient Boosting    0.8416   0.84185    0.83551  0.84829\n",
       "4                  fasttext + SVM    0.8576   0.85788    0.85109  0.86479\n",
       "5  fasttext + Logistic Regression    0.8534   0.85314    0.84956  0.85674\n",
       "6        fasttext + Random Forest    0.8132   0.81585    0.79977  0.83260\n",
       "7    fasttext + Gradient Boosting    0.8272   0.82823    0.81847  0.83823"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the FastText model using classifiers with statistics\n",
    "\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_fasttext.tolist(), y_train)\n",
    "    y_pred = classifier.predict(X_test_fasttext.tolist())\n",
    "    print(f'Classifier: {name}')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.5f}')\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred):.5f}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred):.5f}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred):.5f}')\n",
    "    \n",
    "    # Create a DataFrame with values rounded to 5 decimal places\n",
    "    df = pd.DataFrame([[\n",
    "        \"fasttext + \" + name, \n",
    "        round(accuracy_score(y_test, y_pred), 5), \n",
    "        round(f1_score(y_test, y_pred), 5), \n",
    "        round(precision_score(y_test, y_pred), 5), \n",
    "        round(recall_score(y_test, y_pred), 5)\n",
    "    ]], columns=['Classifier', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "    \n",
    "    results_df = pd.concat([results_df, df], ignore_index=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(r'C:\\NUS\\Y4S1\\Natural Language Processing\\glove model\\glove.6B.100d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the GloVe model\n",
    "def encode_sentence_glove(sentence, glove_embeddings, vector_size=100):\n",
    "    words = sentence.split()\n",
    "    vectors = [glove_embeddings[word] for word in words if word in glove_embeddings]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # If no word is found in GloVe, return a zero vector of the same size\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Encode training and test data\n",
    "X_train_glove = X_train.apply(lambda x: encode_sentence_glove(x, glove_embeddings, vector_size=100))\n",
    "X_test_glove = X_test.apply(lambda x: encode_sentence_glove(x, glove_embeddings, vector_size=100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: SVM\n",
      "Accuracy: 0.79800\n",
      "F1 Score: 0.79768\n",
      "Precision: 0.79418\n",
      "Recall: 0.80121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.79320\n",
      "F1 Score: 0.79178\n",
      "Precision: 0.79242\n",
      "Recall: 0.79115\n",
      "Classifier: Random Forest\n",
      "Accuracy: 0.75760\n",
      "F1 Score: 0.75828\n",
      "Precision: 0.75168\n",
      "Recall: 0.76499\n",
      "Classifier: Gradient Boosting\n",
      "Accuracy: 0.77320\n",
      "F1 Score: 0.77383\n",
      "Precision: 0.76710\n",
      "Recall: 0.78068\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word2vec + SVM</td>\n",
       "      <td>0.8668</td>\n",
       "      <td>0.86780</td>\n",
       "      <td>0.85625</td>\n",
       "      <td>0.87968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word2vec + Logistic Regression</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.86396</td>\n",
       "      <td>0.85411</td>\n",
       "      <td>0.87404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word2vec + Random Forest</td>\n",
       "      <td>0.8364</td>\n",
       "      <td>0.83866</td>\n",
       "      <td>0.82244</td>\n",
       "      <td>0.85553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word2vec + Gradient Boosting</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>0.84185</td>\n",
       "      <td>0.83551</td>\n",
       "      <td>0.84829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fasttext + SVM</td>\n",
       "      <td>0.8576</td>\n",
       "      <td>0.85788</td>\n",
       "      <td>0.85109</td>\n",
       "      <td>0.86479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fasttext + Logistic Regression</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>0.85314</td>\n",
       "      <td>0.84956</td>\n",
       "      <td>0.85674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fasttext + Random Forest</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.81585</td>\n",
       "      <td>0.79977</td>\n",
       "      <td>0.83260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fasttext + Gradient Boosting</td>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.82823</td>\n",
       "      <td>0.81847</td>\n",
       "      <td>0.83823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>glove + SVM</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>0.79768</td>\n",
       "      <td>0.79418</td>\n",
       "      <td>0.80121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>glove + Logistic Regression</td>\n",
       "      <td>0.7932</td>\n",
       "      <td>0.79178</td>\n",
       "      <td>0.79242</td>\n",
       "      <td>0.79115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>glove + Random Forest</td>\n",
       "      <td>0.7576</td>\n",
       "      <td>0.75828</td>\n",
       "      <td>0.75168</td>\n",
       "      <td>0.76499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>glove + Gradient Boosting</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>0.77383</td>\n",
       "      <td>0.76710</td>\n",
       "      <td>0.78068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Classifier  Accuracy  F1 Score  Precision   Recall\n",
       "0                   word2vec + SVM    0.8668   0.86780    0.85625  0.87968\n",
       "1   word2vec + Logistic Regression    0.8632   0.86396    0.85411  0.87404\n",
       "2         word2vec + Random Forest    0.8364   0.83866    0.82244  0.85553\n",
       "3     word2vec + Gradient Boosting    0.8416   0.84185    0.83551  0.84829\n",
       "4                   fasttext + SVM    0.8576   0.85788    0.85109  0.86479\n",
       "5   fasttext + Logistic Regression    0.8534   0.85314    0.84956  0.85674\n",
       "6         fasttext + Random Forest    0.8132   0.81585    0.79977  0.83260\n",
       "7     fasttext + Gradient Boosting    0.8272   0.82823    0.81847  0.83823\n",
       "8                      glove + SVM    0.7980   0.79768    0.79418  0.80121\n",
       "9      glove + Logistic Regression    0.7932   0.79178    0.79242  0.79115\n",
       "10           glove + Random Forest    0.7576   0.75828    0.75168  0.76499\n",
       "11       glove + Gradient Boosting    0.7732   0.77383    0.76710  0.78068"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the GloVe model using classifiers with statistics\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train_glove.tolist(), y_train)\n",
    "    y_pred = classifier.predict(X_test_glove.tolist())\n",
    "    print(f'Classifier: {name}')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.5f}')\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred):.5f}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred):.5f}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred):.5f}')\n",
    "    \n",
    "    # Create a DataFrame with values rounded to 5 decimal places\n",
    "    df = pd.DataFrame([[\n",
    "        \"glove + \" + name, \n",
    "        round(accuracy_score(y_test, y_pred), 5), \n",
    "        round(f1_score(y_test, y_pred), 5), \n",
    "        round(precision_score(y_test, y_pred), 5), \n",
    "        round(recall_score(y_test, y_pred), 5)\n",
    "    ]], columns=['Classifier', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "    \n",
    "    results_df = pd.concat([results_df, df], ignore_index=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results_df to a CSV file at C:\\NUS\\Y4S1\\Natural Language Processing with the name Tutorial_2_results.csv\n",
    "results_df.to_csv(r'C:\\NUS\\Y4S1\\Natural Language Processing\\Tutorial_2_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key insights:\n",
    "Word2Vec shows the highest performance across all metrics, with SVM achieving the best results: 86.68% accuracy and 0.8678 F1 score.\n",
    "FastText comes in a close second, with SVM performing best again with 85.76% accuracy and 0.85788 F1 score. Logistic Regression also performs well on FastText embeddings.\n",
    "GloVe generally underperforms compared to Word2Vec and FastText, with its best result coming from SVM, achieving 79.8% accuracy and 0.79768 F1 score.\n",
    "Random Forest and Gradient Boosting models perform worse overall than SVM and Logistic Regression, especially when paired with GloVe embeddings, which show the lowest scores for these models.\n",
    "\n",
    "\n",
    "## Overall:\n",
    "SVM and Logistic Regression perform best when paired with Word2Vec and FastText embeddings, while GloVe performs notably worse across all machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DistilBertForSequenceClassification Transformer\n",
    "\n",
    "The below code is correct, however it is computationally expensive and takes a long time to run on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3750 [04:25<?, ?it/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even subscribe knee jerk anti free trade politics movie still tired note played clink clink clink even accept preacher peroxide hair advocates return first principles reverend billy pretty hard look serious figure clownish reverend sort wakes every morning aspiration ethereal see face tv climbs back bed night pretty wife admit would take tons save dreary mess movie interminable bus rides worst part progress shown guess colored line moving across map aww guessed oh well virtue short favorable thing say hmmmm yep afraid\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/3750 [03:04<63:07:33, 60.65s/it]"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the training and testing data\n",
    "X_train_encoded = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "X_test_encoded = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        # Convert labels to PyTorch tensors\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # No need to clone tensors; just access them directly\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]  # Access the label directly\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset = TextDataset(X_train_encoded, y_train.values)  # Ensure y_train is a NumPy array or list\n",
    "test_dataset = TextDataset(X_test_encoded, y_test.values)      # Ensure y_test is a NumPy array or list\n",
    "\n",
    "print(len(X_train_encoded['input_ids']), len(X_train_encoded['attention_mask']), len(y_train))\n",
    "\n",
    "X_train_encoded = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "X_test_encoded = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "\n",
    "# Load the DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "# Define the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")\n",
    "print(X_train.iloc[3102])\n",
    "print(y_train.iloc[3102])\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate the LDA model using a Support Vector Classifier\u001b[39;00m\n\u001b[0;32m     11\u001b[0m svc \u001b[38;5;241m=\u001b[39m SVC()\n\u001b[1;32m---> 12\u001b[0m svc\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_lda\u001b[49m, y_train)\n\u001b[0;32m     13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svc\u001b[38;5;241m.\u001b[39mpredict(X_test_lda)\n\u001b[0;32m     14\u001b[0m print_metrics(y_test, y_pred, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDA + SVC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_lda' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the DistilBERT model using classifiers with statistics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
