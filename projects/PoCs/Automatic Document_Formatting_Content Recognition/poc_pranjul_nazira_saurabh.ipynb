{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Processing Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "\n",
    "def detect_file_type(file_path):\n",
    "    \"\"\"\n",
    "    Detect the type of the document based on the file extension.\n",
    "    \"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    if file_extension.lower() == '.pdf':\n",
    "        return 'PDF'\n",
    "    elif file_extension.lower() == '.docx':\n",
    "        return 'DOCX'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please upload a PDF or DOCX file.\")\n",
    "\n",
    "def extract_pdf_content(file_path):\n",
    "    \"\"\"\n",
    "    Extract text and metadata from a PDF file using PyMuPDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        text_content = \"\"\n",
    "        for page_num in range(len(pdf_document)):\n",
    "            page = pdf_document[page_num]\n",
    "            text_content += page.get_text()\n",
    "        pdf_document.close()\n",
    "        return text_content\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error processing PDF file: {e}\")\n",
    "\n",
    "def process_input_file(file_path):\n",
    "    \"\"\"\n",
    "    Detect the file type and extract raw content.\n",
    "    \"\"\"\n",
    "    file_type = detect_file_type(file_path)\n",
    "    if file_type == 'PDF':\n",
    "        return extract_pdf_content(file_path)\n",
    "    elif file_type == 'DOCX':\n",
    "        raise RuntimeError(\"DOCX processing not implemented yet.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from langdetect import detect  # For language detection\n",
    "\n",
    "# Load SpaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text by removing unwanted elements like extra spaces,\n",
    "    headers, footers, and non-informative lines.\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r\"\\n\\s*\\n\", \"\\n\", raw_text)  # Remove multiple newlines\n",
    "    cleaned_text = re.sub(r\"Page\\s\\d+(\\s(of)\\s\\d+)?\", \"\", cleaned_text, flags=re.IGNORECASE)  # Remove page numbers\n",
    "    cleaned_text = re.sub(r\"^\\s*[\\d\\W]+\\s*$\", \"\", cleaned_text, flags=re.MULTILINE)  # Remove non-informative lines\n",
    "    cleaned_text = cleaned_text.strip()  # Remove leading/trailing whitespace\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize_text(cleaned_text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text into sentences and words using SpaCy.\n",
    "    \"\"\"\n",
    "    doc = nlp(cleaned_text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    words = [token.text for token in doc if not token.is_space]\n",
    "    return sentences, words\n",
    "\n",
    "def segment_text(cleaned_text):\n",
    "    \"\"\"\n",
    "    Segments text into sections based on headings or predefined delimiters.\n",
    "    \"\"\"\n",
    "    # Simple heuristic for heading-based segmentation\n",
    "    sections = re.split(r\"\\n[A-Z][^\\n]+:\\n\", cleaned_text)\n",
    "    sections = [section.strip() for section in sections if section.strip()]\n",
    "    return sections\n",
    "\n",
    "def detect_language(cleaned_text):\n",
    "    \"\"\"\n",
    "    Detects the language of the text using the langdetect library.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(cleaned_text)\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def preprocess_text(raw_text, detect_lang=True):\n",
    "    \"\"\"\n",
    "    Integrates cleaning, tokenization, segmentation, and optional language detection.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning text...\")\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "\n",
    "    print(\"Tokenizing text...\")\n",
    "    sentences, words = tokenize_text(cleaned_text)\n",
    "\n",
    "    print(\"Segmenting text...\")\n",
    "    sections = segment_text(cleaned_text)\n",
    "\n",
    "    language = \"Not Detected\"\n",
    "    if detect_lang:\n",
    "        print(\"Detecting language...\")\n",
    "        language = detect_language(cleaned_text)\n",
    "\n",
    "    return {\n",
    "        \"cleaned_text\": cleaned_text,\n",
    "        \"sentences\": sentences,\n",
    "        \"words\": words,\n",
    "        \"sections\": sections,\n",
    "        \"language\": language\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Structure Recognition Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy language model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_title(cleaned_text):\n",
    "    \"\"\"\n",
    "    Extracts the title of the document, assuming it spans multiple lines at the beginning.\n",
    "    \"\"\"\n",
    "    lines = cleaned_text.split(\"\\n\")\n",
    "    title_lines = []\n",
    "    for line in lines:\n",
    "        if len(line.strip()) > 0 and not line.isupper():  # Avoid footers or all-uppercase text\n",
    "            title_lines.append(line.strip())\n",
    "        if len(title_lines) >= 3:  # Assuming title won't exceed 3 lines\n",
    "            break\n",
    "    return \" \".join(title_lines)\n",
    "\n",
    "def extract_abstract(cleaned_text):\n",
    "    \"\"\"\n",
    "    Extracts the abstract using a pattern or keyword match.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"\\bAbstract\\b.*?(?=\\n[A-Z][^\\n]+\\n|$)\", cleaned_text, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(0).strip() if match else \"Abstract not found\"\n",
    "\n",
    "def extract_metadata(cleaned_text):\n",
    "    \"\"\"\n",
    "    Performs Named Entity Recognition (NER) to extract authors, affiliations, and other metadata.\n",
    "    \"\"\"\n",
    "    doc = nlp(cleaned_text)\n",
    "    authors = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "    return {\n",
    "        \"authors\": authors[:5],  # Limit to first 5 authors\n",
    "        \"affiliations\": organizations\n",
    "    }\n",
    "\n",
    "def segment_hierarchy(cleaned_text):\n",
    "    \"\"\"\n",
    "    Segments the document into sections and handles footers separately.\n",
    "    \"\"\"\n",
    "    # Extract footers (e.g., page numbers, footnotes)\n",
    "    footer_pattern = r\"Page\\s\\d+(\\s(of)\\s\\d+)?|Â©|All rights reserved|doi:\"\n",
    "    main_content = re.sub(footer_pattern, \"\", cleaned_text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Identify main sections using common headings (a general approach for first time)\n",
    "    section_patterns = [\n",
    "        r\"\\bAbstract\\b\",\n",
    "        r\"\\bIntroduction\\b\",\n",
    "        r\"\\bMethodology\\b\",\n",
    "        r\"\\bMethods\\b\",\n",
    "        r\"\\bResults\\b\",\n",
    "        r\"\\bDiscussion\\b\",\n",
    "        r\"\\bConclusion\\b\",\n",
    "        r\"\\bReferences\\b\"\n",
    "    ]\n",
    "    sections = re.split(r\"|\".join(section_patterns), main_content, flags=re.IGNORECASE)\n",
    "    headings = re.findall(r\"|\".join(section_patterns), main_content, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Create hierarchical structure\n",
    "    hierarchy = {}\n",
    "    for i, heading in enumerate(headings):\n",
    "        content = sections[i + 1].strip() if i + 1 < len(sections) else \"\"\n",
    "        hierarchy[heading] = content\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "def recognize_structure(cleaned_text):\n",
    "    \"\"\"\n",
    "    Integrates title, metadata, and hierarchical segmentation.\n",
    "    \"\"\"\n",
    "    print(\"Extracting title...\")\n",
    "    title = extract_title(cleaned_text)\n",
    "\n",
    "    print(\"Extracting abstract...\")\n",
    "    abstract = extract_abstract(cleaned_text)\n",
    "\n",
    "    print(\"Extracting metadata...\")\n",
    "    metadata = extract_metadata(cleaned_text)\n",
    "\n",
    "    print(\"Segmenting document into sections...\")\n",
    "    hierarchy = segment_hierarchy(cleaned_text)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"metadata\": metadata,\n",
    "        \"hierarchy\": hierarchy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Script for Integration for modular functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # File path to the PDF document\n",
    "#     file_path = r\"D:\\Voice Assistants\\ai_assistant\\modules\\research_papers\\precision meds for hypertension.pdf\"\n",
    "    \n",
    "#     try:\n",
    "#         # Step 1: Use Input Processing Module to extract raw text\n",
    "#         from input_processing_module import process_input_file\n",
    "#         raw_text = process_input_file(file_path)\n",
    "        \n",
    "#         # Step 2: Use Text Preprocessing Module to clean, tokenize, and segment text\n",
    "#         from text_processing_module import preprocess_text\n",
    "#         processed_data = preprocess_text(raw_text, detect_lang=True)\n",
    "        \n",
    "#         # Step 3: Use Document Structure Recognition Module to identify sections\n",
    "#         from document_structure_module import recognize_structure\n",
    "#         document_structure = recognize_structure(processed_data[\"cleaned_text\"])\n",
    "        \n",
    "#         # Output Results\n",
    "#         print(\"\\nTitle:\")\n",
    "#         print(document_structure[\"title\"])\n",
    "        \n",
    "#         print(\"\\nAbstract:\")\n",
    "#         print(document_structure[\"abstract\"])\n",
    "        \n",
    "#         print(\"\\nMetadata:\")\n",
    "#         print(\"Authors:\", document_structure[\"metadata\"][\"authors\"])\n",
    "#         print(\"Affiliations:\", document_structure[\"metadata\"][\"affiliations\"])\n",
    "        \n",
    "#         print(\"\\nSections:\")\n",
    "#         for heading, content in document_structure[\"hierarchy\"].items():\n",
    "#             print(f\"\\n{heading}:\\n{content[:500]}\")  # Preview first 500 characters of each section\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # File path to the PDF document\n",
    "    file_path = r\"provide path here\" #path to the pdf file\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Use Input Processing Module to extract raw text\n",
    "        # from input_processing_module import process_input_file\n",
    "        raw_text = process_input_file(file_path)\n",
    "        \n",
    "        # Step 2: Use Text Preprocessing Module to clean, tokenize, and segment text\n",
    "        # from text_processing_module import preprocess_text\n",
    "        processed_data = preprocess_text(raw_text, detect_lang=True)\n",
    "        \n",
    "        # Step 3: Use Document Structure Recognition Module to identify sections\n",
    "        # from document_structure_module import recognize_structure\n",
    "        document_structure = recognize_structure(processed_data[\"cleaned_text\"])\n",
    "        \n",
    "        # Output Results\n",
    "        print(\"\\nTitle:\")\n",
    "        print(document_structure[\"title\"])\n",
    "        \n",
    "        print(\"\\nAbstract:\")\n",
    "        print(document_structure[\"abstract\"])\n",
    "        \n",
    "        print(\"\\nMetadata:\")\n",
    "        print(\"Authors:\", document_structure[\"metadata\"][\"authors\"])\n",
    "        print(\"Affiliations:\", document_structure[\"metadata\"][\"affiliations\"])\n",
    "        \n",
    "        print(\"\\nSections:\")\n",
    "        for heading, content in document_structure[\"hierarchy\"].items():\n",
    "            print(f\"\\n{heading}:\\n{content}\")  # Preview \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
