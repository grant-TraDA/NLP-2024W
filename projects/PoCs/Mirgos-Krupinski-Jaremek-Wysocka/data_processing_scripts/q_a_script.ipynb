{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balladyna: 8/100 QA pairs generated\n",
      "Balladyna: 21/100 QA pairs generated\n",
      "Balladyna: 30/100 QA pairs generated\n",
      "Balladyna: 39/100 QA pairs generated\n",
      "Balladyna: 46/100 QA pairs generated\n",
      "Balladyna: 54/100 QA pairs generated\n",
      "Balladyna: 60/100 QA pairs generated\n",
      "Balladyna: 70/100 QA pairs generated\n",
      "Balladyna: 79/100 QA pairs generated\n",
      "Balladyna: 87/100 QA pairs generated\n",
      "Balladyna: 100/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 10/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 22/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 37/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 48/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 63/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 74/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 86/100 QA pairs generated\n",
      "Dziady_(Mickiewicz): 101/100 QA pairs generated\n",
      "Konrad_Wallenrod: 11/100 QA pairs generated\n",
      "Konrad_Wallenrod: 23/100 QA pairs generated\n",
      "Konrad_Wallenrod: 38/100 QA pairs generated\n",
      "Konrad_Wallenrod: 53/100 QA pairs generated\n",
      "Konrad_Wallenrod: 65/100 QA pairs generated\n",
      "Konrad_Wallenrod: 77/100 QA pairs generated\n",
      "Konrad_Wallenrod: 90/100 QA pairs generated\n",
      "Konrad_Wallenrod: 100/100 QA pairs generated\n",
      "Kordian: 11/100 QA pairs generated\n",
      "Kordian: 22/100 QA pairs generated\n",
      "Kordian: 35/100 QA pairs generated\n",
      "Kordian: 48/100 QA pairs generated\n",
      "Kordian: 61/100 QA pairs generated\n",
      "Kordian: 73/100 QA pairs generated\n",
      "Kordian: 86/100 QA pairs generated\n",
      "Kordian: 101/100 QA pairs generated\n",
      "Lalka_(Prus): 15/100 QA pairs generated\n",
      "Lalka_(Prus): 28/100 QA pairs generated\n",
      "Lalka_(Prus): 39/100 QA pairs generated\n",
      "Lalka_(Prus): 50/100 QA pairs generated\n",
      "Lalka_(Prus): 62/100 QA pairs generated\n",
      "Lalka_(Prus): 71/100 QA pairs generated\n",
      "Lalka_(Prus): 80/100 QA pairs generated\n",
      "Lalka_(Prus): 95/100 QA pairs generated\n",
      "Lalka_(Prus): 107/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 13/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 28/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 41/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 54/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 69/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 84/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 94/100 QA pairs generated\n",
      "Ogniem_i_mieczem: 107/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 9/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 20/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 31/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 44/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 53/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 65/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 80/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 93/100 QA pairs generated\n",
      "Pan_Tadeusz_(wyd._1834): 102/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 13/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 28/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 41/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 56/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 67/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 78/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 91/100 QA pairs generated\n",
      "Pan_Wołodyjowski: 106/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 8/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 21/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 32/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 47/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 60/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 69/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 82/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 95/100 QA pairs generated\n",
      "Potop_(Sienkiewicz): 104/100 QA pairs generated\n",
      "Quo_vadis: 9/100 QA pairs generated\n",
      "Quo_vadis: 22/100 QA pairs generated\n",
      "Quo_vadis: 32/100 QA pairs generated\n",
      "Quo_vadis: 44/100 QA pairs generated\n",
      "Quo_vadis: 54/100 QA pairs generated\n",
      "Quo_vadis: 66/100 QA pairs generated\n",
      "Quo_vadis: 79/100 QA pairs generated\n",
      "Quo_vadis: 88/100 QA pairs generated\n",
      "Quo_vadis: 103/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 11/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 23/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 38/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 50/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 62/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 75/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 88/100 QA pairs generated\n",
      "Sonety_Adama_Mickiewicza: 102/100 QA pairs generated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import random\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import List, Dict\n",
    "\n",
    "async def get_qa_from_gpt_async(paragraph: str, session, previous_questions: List[str]) -> List[str]:\n",
    "    async with session.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {openai.api_key}\"},\n",
    "        json={\n",
    "            \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": f\"\"\"Generate 3 question-answer pairs about the following text. Format each pair as 'question|answer'. Both Question and Answer should be in polish. If you can't generate a question, just skip it.\n",
    "Previous questions asked:\n",
    "{chr(10).join(previous_questions) if previous_questions else 'None'}\n",
    "Please generate unique questions that haven't been asked before.\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": paragraph}\n",
    "            ]\n",
    "        }\n",
    "    ) as response:\n",
    "        result = await response.json()\n",
    "        return result['choices'][0]['message']['content'].strip().split('\\n')\n",
    "\n",
    "async def process_files_async(input_dir: str, output_dir: str, batch_size: int = 5, min_qa_pairs: int = 100):\n",
    "    base_dir = Path(output_dir)\n",
    "    questions_dir = base_dir / 'questions'\n",
    "    answers_dir = base_dir / 'answers'\n",
    "    \n",
    "    for dir_path in [questions_dir, answers_dir]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for file_path in Path(input_dir).glob('*.txt'):\n",
    "            file_questions, file_answers = [], []\n",
    "            text = file_path.read_text()\n",
    "            paragraphs = split_into_paragraphs(text)\n",
    "            processed_paragraphs = set()\n",
    "            \n",
    "            while len(file_questions) < min_qa_pairs and (len(processed_paragraphs) < len(paragraphs)):\n",
    "                available_paragraphs = [p for i, p in enumerate(paragraphs) \n",
    "                                     if i not in processed_paragraphs]\n",
    "                \n",
    "                batch = random.sample(available_paragraphs, \n",
    "                                    min(batch_size, len(available_paragraphs)))\n",
    "                \n",
    "                questions, answers = await process_paragraph_batch(batch, session, file_questions)\n",
    "                file_questions.extend(questions)\n",
    "                file_answers.extend(answers)\n",
    "                \n",
    "                for p in batch:\n",
    "                    processed_paragraphs.add(paragraphs.index(p))\n",
    "                \n",
    "                print(f\"{file_path.stem}: {len(file_questions)}/{min_qa_pairs} QA pairs generated\")\n",
    "                \n",
    "                if len(processed_paragraphs) == len(paragraphs) and len(file_questions) < min_qa_pairs:\n",
    "                    print(f\"Warning: Could not generate {min_qa_pairs} QA pairs from {file_path.stem}\")\n",
    "                    break\n",
    "            \n",
    "            if file_questions:\n",
    "                (questions_dir / f\"{file_path.stem}_questions.txt\").write_text('\\n'.join(file_questions))\n",
    "                (answers_dir / f\"{file_path.stem}_answers.txt\").write_text('\\n'.join(file_answers))\n",
    "\n",
    "async def process_paragraph_batch(paragraphs: List[str], session, previous_questions: List[str]) -> tuple[List[str], List[str]]:\n",
    "    questions, answers = [], []\n",
    "    tasks = [get_qa_from_gpt_async(p, session, previous_questions) for p in paragraphs]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    for qa_pairs in results:\n",
    "        if isinstance(qa_pairs, Exception):\n",
    "            continue\n",
    "        for pair in qa_pairs:\n",
    "            if '|' not in pair or pair.count('|') > 1:\n",
    "                continue\n",
    "            q, a = pair.split('|')\n",
    "            q, a = q.strip(), a.strip()\n",
    "            if q and a and q not in previous_questions:\n",
    "                questions.append(q)\n",
    "                answers.append(a)\n",
    "    \n",
    "    return questions, answers\n",
    "\n",
    "# Usage\n",
    "with open('../../secret.txt') as f:\n",
    "    openai.api_key = f.read().strip()\n",
    "#openai.api_key = ''\n",
    "await process_files_async('../rawdata', 'q_a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
