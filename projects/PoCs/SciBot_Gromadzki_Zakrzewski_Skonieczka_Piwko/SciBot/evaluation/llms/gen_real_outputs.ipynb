{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain.chains import (create_history_aware_retriever,\n",
    "                              create_retrieval_chain)\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "class SciBot:\n",
    "    def __init__(self, llm: str) -> None:\n",
    "        self.store = {}\n",
    "\n",
    "        self.llm = ChatOllama(model=llm)\n",
    "        # ===============================================\n",
    "\n",
    "        ### Contextualize question ###\n",
    "        self.contextualize_q_system_prompt = (\n",
    "            \"Given a chat history and the latest user question \"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question which can be understood \"\n",
    "            \"without the chat history. Do NOT answer the question, \"\n",
    "            \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        )\n",
    "\n",
    "        ### Answer question ###\n",
    "        self.system_prompt = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Use the following pieces of retrieved context to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know. Use three sentences maximum and keep the \"\n",
    "            \"answer concise.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "\n",
    "    def ingest(self, db_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load the database and create the conversational chain.\n",
    "        \"\"\"\n",
    "        model = \"hkunlp/instructor-xl\"\n",
    "        kwargs = {\"device\": \"cpu\"}\n",
    "        embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=model,\n",
    "            model_kwargs=kwargs,\n",
    "        )\n",
    "\n",
    "        db = FAISS.load_local(\n",
    "            folder_path=db_path,\n",
    "            index_name=\"faiss_index\",\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "\n",
    "        self.retriever = db.as_retriever(\n",
    "            search_type=\"mmr\",  # “similarity” (default), “mmr”, or “similarity_score_threshold”\n",
    "            search_kwargs={\"k\": 6},\n",
    "        )\n",
    "\n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        history_aware_retriever = create_history_aware_retriever(\n",
    "            self.llm, self.retriever, contextualize_q_prompt\n",
    "        )\n",
    "\n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)\n",
    "\n",
    "        self.rag_chain = create_retrieval_chain(\n",
    "            history_aware_retriever, self.question_answer_chain\n",
    "        )\n",
    "\n",
    "        self.conversational_rag_chain = RunnableWithMessageHistory(\n",
    "            self.rag_chain,\n",
    "            self.get_session_history,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            output_messages_key=\"answer\",\n",
    "        )\n",
    "\n",
    "    def get_session_history(self, session_id: str) -> ChatMessageHistory:\n",
    "        \"\"\"\n",
    "        Get the chat history for a given session ID.\n",
    "        \"\"\"\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "\n",
    "    def ask(self, query: str, session_id: str = \"abc123\") -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Ask a question and get a response.\n",
    "        \"\"\"\n",
    "        response = self.conversational_rag_chain.invoke(\n",
    "            {\"input\": query},\n",
    "            config={\n",
    "                \"configurable\": {\"session_id\": session_id},\n",
    "            },\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/llm_eval/questions.txt\", \"r\") as f:\n",
    "    questions = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"qwen2.5:3b\", \"qwen2.5:7b-instruct-q4_0\", \"llama3.1:latest\", \"llama3.2:latest\"]\n",
    "db_path = \"../../data/dbs/db_instructor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majkel/miniconda3/lib/python3.12/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majkel/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/majkel/miniconda3/lib/python3.12/site-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:45<00:00, 11.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majkel/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/majkel/miniconda3/lib/python3.12/site-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [13:17<00:00, 31.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majkel/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/majkel/miniconda3/lib/python3.12/site-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [13:10<00:00, 31.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majkel/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/majkel/miniconda3/lib/python3.12/site-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [03:16<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for llm in llms:\n",
    "    chat = SciBot(llm=llm)\n",
    "    chat.ingest(db_path)\n",
    "    sample_ans = chat.ask(\"What is the capital of France?\") # Sample question to eliminate cold start\n",
    "    \n",
    "    for i, question in enumerate(tqdm(questions)):\n",
    "        start = time.time()\n",
    "        response = chat.ask(question, session_id=str(i))\n",
    "        ex_time = time.time() - start\n",
    "        context = \"\".join(f\"Document {i+1}: \\n {doc.page_content} \\n\\n\" for i, doc in enumerate(response[\"context\"]))\n",
    "        results.append([llm, question, context, response[\"answer\"], ex_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>ex_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>What does the term \"learn to optimize\" mean?\\n</td>\n",
       "      <td>Document 1: \\n NatlSciRev ,2024,Vol.11,nwae132...</td>\n",
       "      <td>The term \"learn to optimize\" (L2O) refers to a...</td>\n",
       "      <td>9.534233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>Please give some examples of metaheuristics.\\n</td>\n",
       "      <td>Document 1: \\n usually provide only sub-optima...</td>\n",
       "      <td>Metaheuristics include various algorithms insp...</td>\n",
       "      <td>9.287290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>What is the \"no free lunch\" theorem about?\\n</td>\n",
       "      <td>Document 1: \\n IEEE TRANSACTIONS ON EVOLUTIONA...</td>\n",
       "      <td>The \"no free lunch\" (NFL) theorem states that ...</td>\n",
       "      <td>7.663871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>What is the concept behind Parallel Algorithm ...</td>\n",
       "      <td>Document 1: \\n training set as well as for con...</td>\n",
       "      <td>A Parallel Algorithm Portfolio (AAP) is a coll...</td>\n",
       "      <td>8.298491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>Please provide some approaches to how Parallel...</td>\n",
       "      <td>Document 1: \\n algorithms, and thereby combine...</td>\n",
       "      <td>To construct a parallel algorithm portfolio, s...</td>\n",
       "      <td>11.127954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>What are the main benefits of AAC?\\n</td>\n",
       "      <td>Document 1: \\n as expected, as N and K get lar...</td>\n",
       "      <td>Augmentative and Alternative Communication (AA...</td>\n",
       "      <td>12.816970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>Please provide an overview of the types of AAC...</td>\n",
       "      <td>Document 1: \\n the best solver in the solver c...</td>\n",
       "      <td>There are several Augmentative and Alternative...</td>\n",
       "      <td>14.187720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>Are Parallel Algorithm Portfolios connected wi...</td>\n",
       "      <td>Document 1: \\n algorithms, and thereby combine...</td>\n",
       "      <td>Yes, the concept of parallel algorithm portfol...</td>\n",
       "      <td>6.678970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>What factors and challenges influenced the dev...</td>\n",
       "      <td>Document 1: \\n training set as well as for con...</td>\n",
       "      <td>The development of the Parallel Algorithm Port...</td>\n",
       "      <td>13.886470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>What are the main benefits of PAPs?</td>\n",
       "      <td>Document 1: \\n independently in parallel to ob...</td>\n",
       "      <td>PAPs (Personal Air Purifiers) offer several be...</td>\n",
       "      <td>9.940728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                llm                                           question  \\\n",
       "0        qwen2.5:3b     What does the term \"learn to optimize\" mean?\\n   \n",
       "1        qwen2.5:3b     Please give some examples of metaheuristics.\\n   \n",
       "2        qwen2.5:3b       What is the \"no free lunch\" theorem about?\\n   \n",
       "3        qwen2.5:3b  What is the concept behind Parallel Algorithm ...   \n",
       "4        qwen2.5:3b  Please provide some approaches to how Parallel...   \n",
       "..              ...                                                ...   \n",
       "95  llama3.2:latest               What are the main benefits of AAC?\\n   \n",
       "96  llama3.2:latest  Please provide an overview of the types of AAC...   \n",
       "97  llama3.2:latest  Are Parallel Algorithm Portfolios connected wi...   \n",
       "98  llama3.2:latest  What factors and challenges influenced the dev...   \n",
       "99  llama3.2:latest                What are the main benefits of PAPs?   \n",
       "\n",
       "                                              context  \\\n",
       "0   Document 1: \\n NatlSciRev ,2024,Vol.11,nwae132...   \n",
       "1   Document 1: \\n usually provide only sub-optima...   \n",
       "2   Document 1: \\n IEEE TRANSACTIONS ON EVOLUTIONA...   \n",
       "3   Document 1: \\n training set as well as for con...   \n",
       "4   Document 1: \\n algorithms, and thereby combine...   \n",
       "..                                                ...   \n",
       "95  Document 1: \\n as expected, as N and K get lar...   \n",
       "96  Document 1: \\n the best solver in the solver c...   \n",
       "97  Document 1: \\n algorithms, and thereby combine...   \n",
       "98  Document 1: \\n training set as well as for con...   \n",
       "99  Document 1: \\n independently in parallel to ob...   \n",
       "\n",
       "                                               answer    ex_time  \n",
       "0   The term \"learn to optimize\" (L2O) refers to a...   9.534233  \n",
       "1   Metaheuristics include various algorithms insp...   9.287290  \n",
       "2   The \"no free lunch\" (NFL) theorem states that ...   7.663871  \n",
       "3   A Parallel Algorithm Portfolio (AAP) is a coll...   8.298491  \n",
       "4   To construct a parallel algorithm portfolio, s...  11.127954  \n",
       "..                                                ...        ...  \n",
       "95  Augmentative and Alternative Communication (AA...  12.816970  \n",
       "96  There are several Augmentative and Alternative...  14.187720  \n",
       "97  Yes, the concept of parallel algorithm portfol...   6.678970  \n",
       "98  The development of the Parallel Algorithm Port...  13.886470  \n",
       "99  PAPs (Personal Air Purifiers) offer several be...   9.940728  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results, columns=[\"llm\", \"question\", \"context\", \"answer\", \"ex_time\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/llm_eval/real_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
